---
output: pdf_document
header-includes:
   - \usepackage{tcolorbox}
   - \usepackage{fancyhdr}
   - \usepackage[utf8]{inputenc}
urlcolor: blue
---

\pagestyle{fancy}
\fancyhf{}
\rhead{Will Nickols}
\lhead{Section 10}
\rfoot{Page \thepage}

# Announcements
- Make sure to sign in on the [google form](https://forms.gle/JGvZP8CPUhaefnLT6)
- Pset 9 due December 2 at 5 pm
- Final project due Wednesday December 14th at 5 pm

```{r, echo=F, warning=F, message=F}
library(lme4)
library(ggplot2)
library(reshape2)
library(gridExtra)
library(dplyr)
library(MASS)
library(nlme)
```

# Many ways to ~~skin a cat~~ [peel an orange](https://www.peta.org/features/animal-friendly-idioms/)

Recall the following linear model extensions:

- Heteroscedasticity-consistent standard errors: $\vec{Y}=\mathbf{X}\vec{\beta}+\vec{\epsilon}$, $\epsilon\sim MVN(\vec{0}, \vec{\sigma^2}\mathbf{I}_{n\times n})$ where $\vec{\sigma}^2$ has non-identical entries
- Weighted least squares: $\vec{Y}=\mathbf{X}\vec{\beta}+\vec{\epsilon}$, $\epsilon\sim MVN(\vec{0},\mathbf{W}^{-1}\sigma^2\mathbf{I}_{n\times n})$ where $\mathbf{W}$ is a diagonal matrix of weights
- Huber's method: Minimize $\textrm{loss}=\sum_{i}^ng(\hat\epsilon_i)$ with $$g(x)=\begin{cases} 
      x^2/2 & |x|<c \\
      c|x| - c^2/2 & |x|\geq c
      \end{cases}$$
- Block correlations: $\vec{Y}=\mathbf{X}\vec{\beta}+\vec{\epsilon}$, $\epsilon\sim MVN(\vec{0}, \mathbf{\Sigma})$ with a covariance matrix like $$\mathbf{\Sigma}=\begin{bmatrix}
1 & \rho_1 & \rho_1 & 0 & 0 \\
\rho_1 & 1 & \rho_1 & 0 & 0 \\
\rho_1 & \rho_1 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & \rho_2 \\
0 & 0 & 0 & \rho_2 & 1 \\
\end{bmatrix}	$$
- Autoregressive: $\vec{Y}=\mathbf{X}\vec{\beta}+\vec{\epsilon}$, $\epsilon\sim MVN(\vec{0}, \mathbf{\Sigma})$ with a covariance matrix like $$\mathbf{\Sigma}=\begin{bmatrix}
1 & \rho & \rho^2 & \rho^3 & \rho^4 \\
\rho & 1 & \rho & \rho^2 & \rho^3 \\
\rho^2 & \rho & 1 & \rho & \rho^2 \\
\rho^3 & \rho^2 & \rho & 1 & \rho \\
\rho^4 & \rho^3 & \rho^2 & \rho & 1 \\
\end{bmatrix}	$$
- Random intercepts: $Y_{ij}\sim\alpha + \alpha_i+\beta X_{ij}+\epsilon_{ij}$ with $\epsilon_{ij}\sim\mathcal{N}(0, \sigma^2_Y)$ and $\alpha_i\sim\mathcal{N}(0, \sigma^2_\alpha)$.  (Note that this version explicitly includes a fixed slope and a random slope with mean 0.  Random effects as you'll see them elsewhere (and in `lmer`) are usually specified to have mean 0, so including the fixed effect separately is a good practice.)
- Random slopes: $Y_{ij}\sim\alpha +(\beta + \beta_i)X_{ij}+\epsilon_{ij}$ with $\epsilon_{ij}\sim\mathcal{N}(0, \sigma^2_Y)$ and $\beta_i\sim\mathcal{N}(0, \sigma^2_\beta)$.

1. For each of the scenarios below, which of the above linear model extensions would you use?

- Stock price over a week: Autoregressive
- Wheat yield vs phosphorus fertilizer use in various Kansas counties: Block correlation, random intercepts, random slopes
- Soil nitrate concentration vs fertilizer use where the nitrate measurements are taken with instruments of varying precision: Heteroscedasticity-consistent standard errors or weighted regression
- Spotify listens vs release year: Huber's method
- Day in year leaves start to fall vs latitude for various tree types: Block correlation, random intercepts, random slopes

2. Which line is which in the following models?

- One line is a standard OLS; the other uses weighted regression.

```{r, echo=F, warning=F}
set.seed(1)
x <- runif(100,0,10)
y <- c(x[1:50] + rnorm(50, 0, 1), 2 * x[51:100] + rnorm(50, 0, 5))
cols <- c(rep("Low variance", 50), rep("High variance", 50))
weights <- c(rep(1/1, 50), rep(1/5, 50))
df <- data.frame(x, y, Variance=cols, weights)

lm1 <- lm(y ~ x, weights = weights, df)

dummy_x <- x
dummy_y <- predict(lm1)
dummy_df <- data.frame(dummy_x, dummy_y, Variance=cols)

plot1 <- ggplot(df, aes(y=y, x=x, col=Variance)) + 
  geom_point() + 
  geom_line(data=dummy_df, aes(y=dummy_y, x=dummy_x), col="blue", size=1) + 
  geom_smooth(method = "lm", aes(group=1), se = F, col="red", size=1)

saveRDS(plot1, "plot1.RDS")
```
```{r, message=F, echo=F, fig.height=3, fig.width=6}
readRDS(file = "plot1.RDS")
```

Blue is the weighted regression.  It weights low variance observations more heavily as expected.

- One line is a standard OLS; the other uses Huber's method.

```{r, echo=F, warning=F}
set.seed(1)
x <- sort(runif(100,0,10))
y <- c(x + rnorm(100, 0, 1))
y[1:10] <- y[1:10] - rexp(10, 1/3)
y[91:100] <- y[91:100] + rexp(10, 1/3)
df <- data.frame(x, y)

rlm1 = rlm(y ~ x, df)

dummy_x <- x
dummy_y <- predict(rlm1)
dummy_df <- data.frame(dummy_x, dummy_y)

plot2 <- ggplot(df, aes(y=y, x=x)) + 
  geom_point() + 
  geom_line(data=dummy_df, aes(y=dummy_y, x=dummy_x), col="blue", size=1) + 
  geom_smooth(method = "lm", aes(group=1), se = F, col="red", size=1)

saveRDS(plot2, "plot2.RDS")
```

```{r, message=F, echo=F, fig.height=3, fig.width=6}
readRDS(file = "plot2.RDS")
```

Blue is Huber's method.  It is not as sensitive to outliers.

- One line is a standard OLS; the other uses autoregression.

```{r, echo=F, warning=F}
set.seed(2)
x <- sort(runif(100,0,10))
rho = 0.9
exponent <- abs(matrix(1:100 - 1, nrow = 100, ncol = 100, byrow = TRUE) -  (1:100 - 1))
Sigma = rho^exponent
y <- mvrnorm(1, x, Sigma)
df <- data.frame(x, y, id=1:100)

gls1 = gls(y ~ x, df, cor=corAR1(form = ~id))

dummy_x <- x
dummy_y <- predict(gls1)
dummy_df <- data.frame(dummy_x, dummy_y)

plot3 <- ggplot(df, aes(y=y, x=x)) + 
  geom_point() + 
  geom_line(data=dummy_df, aes(y=dummy_y, x=dummy_x), col="red", size=1) + 
  geom_smooth(method = "lm", aes(group=1), se = F, col="blue", size=1)

saveRDS(plot3, "plot3.RDS")
```

```{r, message=F, echo=F, fig.height=3, fig.width=6}
readRDS(file = "plot3.RDS")
```

Red is the autoregression.  It's less influenced by tightly correlated nearby points.

\newpage

# Meta-analysis analysis

One common use of mixed effects models is in meta-analysis to aggregate the results of multiple studies.  This question will explore various methods of meta-analysis aggregation.

1. A common plot in meta-analysis is a forest plot, showing the mean and standard deviation of some quantity of interest as determined by various studies as well as an aggregated mean and variance.  Given the following forest plots (technically these are axis-flipped forest plots), determine what a linear mixed effects model combining the studies should look like.

```{r, cache=T, echo=F}
# Version 1

set.seed(1)

true_effect = 1
n_studies = 9

# Effects observed in each study
study_effects = rnorm(n_studies, true_effect, 0.1)

# Number of participants in each study
study_ns = floor(rpois(n_studies, 50))

# Stores all x and y observations and the study they come from
df = matrix(nrow = 0, ncol = 3)

# Stores mean, sd, n for each study
stats_df = matrix(nrow = n_studies, ncol = 4)

for (i in 1:n_studies) {
  study_n = study_ns[i]
  xs = rnorm(study_n, 0, 1)
  ys = rnorm(study_n, xs * study_effects[i], 2)
  df = rbind(df, cbind(xs, ys, study = rep(i, study_n)))
  stats_df[i, 1:2] = summary(lm(ys ~ xs))$coefficients[2, 1:2]
  stats_df[i, 3] = study_n
}
stats_df[1:nrow(stats_df), 4] = 1:n_studies

df = data.frame(df)

# Fit meta-analysis lmer
lmer1 = suppressMessages(lmer(ys ~ xs + (xs - 1 || study), data = df))

stats_df = data.frame(stats_df)
colnames(stats_df) = c("mean", "sd", "n", "study")
stats_df$study <- as.character(stats_df$study)

stats_df[nrow(stats_df) + 1, ] = c(summary(lmer1)$coefficients[2, 1:2], NA, NA)
stats_df[nrow(stats_df), 4] = "Mixed effects"

plot1 <- ggplot(stats_df, aes(x=study, y=mean)) + 
  geom_point()+
  geom_errorbar(aes(ymin=mean-2*sd, ymax=mean+2*sd), width=.2,
                position=position_dodge(0.05)) + 
  ylab("Mean +/- 2 StDev") + 
  xlab("Study")

# Version 2

# Effects observed in each study
study_effects = rnorm(n_studies, true_effect, 2)

# Number of participants in each study
study_ns = floor(rpois(n_studies, 50))

# Stores all x and y observations and the study they come from
df = matrix(nrow = 0, ncol = 3)

# Stores mean, sd, n for each study
stats_df = matrix(nrow = n_studies, ncol = 4)

for (i in 1:n_studies) {
  study_n = study_ns[i]
  xs = rnorm(study_n, 0, 1)
  ys = rnorm(study_n, xs * study_effects[i], 1)
  df = rbind(df, cbind(xs, ys, study = rep(i, study_n)))
  stats_df[i, 1:2] = summary(lm(ys ~ xs))$coefficients[2, 1:2]
  stats_df[i, 3] = study_n
}
stats_df[1:nrow(stats_df), 4] = 1:n_studies

df = data.frame(df)

# Fit meta-analysis lmer
lmer1 = suppressMessages(lmer(ys ~ xs + (xs - 1 || study), data = df))

stats_df = data.frame(stats_df)
colnames(stats_df) = c("mean", "sd", "n", "study")
stats_df$study <- as.character(stats_df$study)

stats_df[nrow(stats_df) + 1, ] = c(summary(lmer1)$coefficients[2, 1:2], NA, NA)
stats_df[nrow(stats_df), 4] = "Mixed effects"

plot2 <- ggplot(stats_df, aes(x=study, y=mean)) + 
  geom_point()+
  geom_errorbar(aes(ymin=mean-2*sd, ymax=mean+2*sd), width=.2,
                position=position_dodge(0.05)) + 
  ylab("Mean +/- 2 StDev") + 
  xlab("Study")

grid.arrange(plot1, plot2, nrow=2)
```

2. Sometimes, studies will not make available all their individual data points.  In these cases, we might only have a point estimate and standard error for a particular statistic of interest.  Assume that for study $i$ we have our statistic of interest $$\hat\beta_i\sim\mathcal{N}(\beta,\sigma_{\hat\beta_i}^2)$$ where we will approximate the true standard error $\sigma_{\hat\beta_i}^2$ with our estimated standard error $\hat\sigma_{\hat\beta_i}^2$.  This model specifies that there is some underlying effect $\beta$ and that each study will find some $\hat\beta_i$ with a standard error based on the study's sample size etc.  Assuming we have $n_{\textrm{studies}}$, use maximum likelihood estimation to find a point estimate for $\beta$.

Keeping only the pieces that depend on $\beta$, 
$$L\propto\prod_{i}^{n_{\textrm{studies}}}e^{\frac{1}{2}(\frac{\hat\beta_i-\beta}{\sigma_{\hat\beta_i}})^2}\implies \log(L)=\sum_{i}^{n_{\textrm{studies}}}\frac{1}{2}\left(\frac{\hat\beta_i-\beta}{\sigma_{\hat\beta_i}}\right)^2$$

Taking the derivative and setting it equal to 0 shows $$0=\sum_{i}^{n_{\textrm{studies}}}\left(\frac{\hat\beta_i-\beta}{\sigma_{\hat\beta_i}}\right)=\sum_{i}^{n_{\textrm{studies}}}\left(\frac{\hat\beta_i}{\sigma_{\hat\beta_i}}\right)-\beta\sum_{i}^{n_{\textrm{studies}}}\left(\frac{1}{\sigma_{\hat\beta_i}}\right)\implies \beta = \frac{\sum_{i}^{n_{\textrm{studies}}}\left(\frac{\hat\beta_i}{\sigma_{\hat\beta_i}}\right)}{\sum_{i}^{n_{\textrm{studies}}}\left(\frac{1}{\sigma_{\hat\beta_i}}\right)}$$

3. The code below simulates data from one of two scenarios.  In the first scenario, there is a true effect $\beta$ that holds in all the studies.  The only thing that differs by study is the sample size.  Therefore, for study $i$, the $j^{th}$ observation is given by $Y_{ij}=\beta_0 + \beta X_{ij}+\epsilon_{ij}$ with $\epsilon_{ij}\sim\mathcal{N}(0, \sigma_Y^2)$.  In the second scenario, the effect in study $i$ is $\beta_i\sim\mathcal{N}(\beta, \sigma_\beta^2)$.  Then, for study $i$, the $j^{th}$ observation is given by $Y_{ij}=\beta_0 + \beta_i X_{ij}+\epsilon_{ij}$ with $\epsilon_{ij}\sim\mathcal{N}(0, \sigma_Y^2)$.

We will consider 5 methods:

- Use the likelihood method above
- Find $\hat\beta_i$ for each study and take the average to estimate $\beta$.
- Use a mixed effects model `ys ~ xs + (xs - 1 | study)` (random slopes, no random intercepts)
- Combine all the data points and run a linear model with `ys ~ xs`
- Combine all the data points and run a linear model with `ys ~ xs + xs:as.factor(study_id)`

Interpret the outputs: how do the methods perform on the two scenarios? (The code below should be cached and takes a while to run, so don't change it unnecessarily.)
  
```{r, warning=F, cache=T, echo=F}
set.seed(139)

nsims = 1000

output_df = matrix(nrow = nsims, ncol = 5)

true_effect = 1
n_studies = 9
b_0 = 120

for (nsim in 1:nsims) {
  # size of studies
  study_ns = floor(rexp(n_studies, 1/50)) + 10
  
  df = matrix(nrow = 0, ncol = 3)
  stats_df = matrix(nrow = n_studies, ncol = 4)
  
  for (i in 1:n_studies) {
    study_n = study_ns[i]
    xs = rnorm(study_n, 0, 1)
    ys = b_0 + rnorm(study_n, xs * true_effect, 10)
    df = rbind(df, cbind(xs, ys, study = rep(i, study_n)))
    stats_df[i, 1:2] = summary(lm(ys ~ xs))$coefficients[2, 1:2]
    stats_df[i, 3] = study_n
  }
  stats_df[1:nrow(stats_df), 4] = 1:n_studies
  
  df = data.frame(df)
  
  lmer1 = suppressMessages(lmer(ys ~ xs + (xs - 1 | study), data = df))

  stats_df = data.frame(stats_df)
  colnames(stats_df) = c("mean", "sd", "n", "study")
  stats_df$study <- as.character(stats_df$study)
  
  stats_df[nrow(stats_df) + 1, ] = list(sum(stats_df$mean/stats_df$sd)/sum(1/stats_df$sd), NA, NA, NA)
  stats_df[nrow(stats_df), 4] = "Likelihood"
  
  stats_df[nrow(stats_df) + 1, ] = list(mean(stats_df$mean), NA, NA, NA)
  stats_df[nrow(stats_df), 4] = "Raw means"
  
  stats_df[nrow(stats_df) + 1, ] = c(summary(lmer1)$coefficients[2, 1:2], NA, NA)
  stats_df[nrow(stats_df), 4] = "Mixed effects"
  
  stats_df[nrow(stats_df) + 1, ] = c(summary(lm(ys ~ xs + xs:as.factor(study), data = df))$coefficients[2, 1:2], NA, NA)
  stats_df[nrow(stats_df), 4] = "Fixed effects with interactions"
  
  stats_df[nrow(stats_df) + 1, ] = c(summary(lm(ys ~ xs, data = df))$coefficients[2, 1:2], NA, NA)
  stats_df[nrow(stats_df), 4] = "Fixed effects no interactions"
  
  output_df[nsim, ] = stats_df[n_studies + (1:5), 1]
  
  # if (nsim %% 100 == 0) {
  #   print(paste0(nsim/nsims * 100, "% done"))
  # }
}

output_df = melt(output_df)
colnames(output_df) <- c("Index", "Method", "Beta")
output_df$Method <- case_when(output_df$Method == 1 ~ "1. Likelihood",
                              output_df$Method == 2 ~ "2. Raw means",
                              output_df$Method == 3 ~ "3. Linear mixed effects",
                              output_df$Method == 4 ~ "4. Fixed effects with interaction",
                              output_df$Method == 5 ~ "5. Fixed effects no interaction")

#ggplot(output_df, aes(x=Beta, fill=Method)) + 
#  geom_histogram(alpha=0.5, position="identity", binwidth = 0.1)

comparison_df = data.frame(matrix(nrow = 5, ncol = 4))
comparison_df[, 1] = c("Likelihood", "Raw means", "Linear mixed effects", "Fixed effects with interaction", "Fixed effects no interaction")
comparison_df[, 2] = as.numeric(by(output_df$Beta, output_df$Method, mean))
comparison_df[, 3] = as.numeric(by(output_df$Beta, output_df$Method, sd))
comparison_df[, 4] = c(sum((output_df$Beta[output_df$Method=="1. Likelihood"] - true_effect)^2),
                       sum((output_df$Beta[output_df$Method=="2. Raw means"] - true_effect)^2),
                       sum((output_df$Beta[output_df$Method=="3. Linear mixed effects"] - true_effect)^2),
                       sum((output_df$Beta[output_df$Method=="4. Fixed effects with interaction"] - true_effect)^2),
                       sum((output_df$Beta[output_df$Method=="5. Fixed effects no interaction"] - true_effect)^2)) / nsims
colnames(comparison_df) <- c("Method", "Mean", "SD", "MSE")

comparison_df
```

```{r, warning=F, cache=T, echo=F}
set.seed(139)

nsims = 1000

output_df = matrix(nrow = nsims, ncol = 5)

true_effect = 1
n_studies = 9
b_0 = 120

for (nsim in 1:nsims) {
  # beta_i
  study_effects = rnorm(n_studies, true_effect, 2)
  
  # size of studies
  study_ns = floor(rexp(n_studies, 1/50)) + 10
  
  df = matrix(nrow = 0, ncol = 3)
  stats_df = matrix(nrow = n_studies, ncol = 4)
  
  for (i in 1:n_studies) {
    study_n = study_ns[i]
    xs = rnorm(study_n, 0, 1)
    ys = b_0 + rnorm(study_n, xs * study_effects[i], 10)
    df = rbind(df, cbind(xs, ys, study = rep(i, study_n)))
    stats_df[i, 1:2] = summary(lm(ys ~ xs))$coefficients[2, 1:2]
    stats_df[i, 3] = study_n
  }
  stats_df[1:nrow(stats_df), 4] = 1:n_studies
  
  df = data.frame(df)
  
  lmer1 = suppressMessages(lmer(ys ~ xs + (xs - 1 | study), data = df))

  stats_df = data.frame(stats_df)
  colnames(stats_df) = c("mean", "sd", "n", "study")
  stats_df$study <- as.character(stats_df$study)
  
  stats_df[nrow(stats_df) + 1, ] = list(sum(stats_df$mean/stats_df$sd)/sum(1/stats_df$sd), NA, NA, NA)
  stats_df[nrow(stats_df), 4] = "Likelihood"
  
  stats_df[nrow(stats_df) + 1, ] = list(mean(stats_df$mean), NA, NA, NA)
  stats_df[nrow(stats_df), 4] = "Raw means"
  
  stats_df[nrow(stats_df) + 1, ] = c(summary(lmer1)$coefficients[2, 1:2], NA, NA)
  stats_df[nrow(stats_df), 4] = "Mixed effects"
  
  stats_df[nrow(stats_df) + 1, ] = c(summary(lm(ys ~ xs + xs:as.factor(study), data = df))$coefficients[2, 1:2], NA, NA)
  stats_df[nrow(stats_df), 4] = "Fixed effects with interactions"
  
  stats_df[nrow(stats_df) + 1, ] = c(summary(lm(ys ~ xs, data = df))$coefficients[2, 1:2], NA, NA)
  stats_df[nrow(stats_df), 4] = "Fixed effects no interactions"
  
  output_df[nsim, ] = stats_df[n_studies + (1:5), 1]
  
  # if (nsim %% 100 == 0) {
  #   print(paste0(nsim/nsims * 100, "% done"))
  # }
}

output_df = melt(output_df)
colnames(output_df) <- c("Index", "Method", "Beta")
output_df$Method <- case_when(output_df$Method == 1 ~ "1. Likelihood",
                              output_df$Method == 2 ~ "2. Raw means",
                              output_df$Method == 3 ~ "3. Linear mixed effects",
                              output_df$Method == 4 ~ "4. Fixed effects with interaction",
                              output_df$Method == 5 ~ "5. Fixed effects no interaction")

#ggplot(output_df, aes(x=Beta, fill=Method)) + 
#  geom_histogram(alpha=0.5, position="identity", binwidth = 0.1)

comparison_df = data.frame(matrix(nrow = 5, ncol = 4))
comparison_df[, 1] = c("Likelihood", "Raw means", "Linear mixed effects", "Fixed effects with interaction", "Fixed effects no interaction")
comparison_df[, 2] = as.numeric(by(output_df$Beta, output_df$Method, mean))
comparison_df[, 3] = as.numeric(by(output_df$Beta, output_df$Method, sd))
comparison_df[, 4] = c(sum((output_df$Beta[output_df$Method=="1. Likelihood"] - true_effect)^2),
                       sum((output_df$Beta[output_df$Method=="2. Raw means"] - true_effect)^2),
                       sum((output_df$Beta[output_df$Method=="3. Linear mixed effects"] - true_effect)^2),
                       sum((output_df$Beta[output_df$Method=="4. Fixed effects with interaction"] - true_effect)^2),
                       sum((output_df$Beta[output_df$Method=="5. Fixed effects no interaction"] - true_effect)^2)) / nsims
colnames(comparison_df) <- c("Method", "Mean", "SD", "MSE")

comparison_df
```

The fixed effects with interactions is clearly the worst model: it actually just takes study 1 as the estimated effect and fits adjustments for the rest of the studies.  The raw means also does rather poorly with a higher standard error than the other methods.  Fixed effects with no interactions does the best when all the participants in all the studies are from the same distribution, but its standard error is higher when there are by-study differences.  Linear mixed effects performs very well in both scenarios, and the likelihood method also does rather well considering that it doesn't have any of the original data points.
  
4. Under the random slopes model specified above but assuming an intercept of 0, given a new $X$ from a new study, find the probability that its associated $Y$ value will be less than $\tau$. You can leave your answer as a double integral.

Conditioning on the group $\beta_i$ and applying the Law of Total Probability,

$$\begin{aligned}
P(Y<\tau)&=\int_{-\infty}^\infty P(Y<\tau|\beta_i)P(\beta_i)d\beta_i\\
&= \int_{-\infty}^\infty \int_{-\infty}^\tau\frac{1}{\sqrt{2\pi}\sigma_Y}e^{-\frac{1}{2}\left(\frac{y-X\beta_i}{\sigma_Y}\right)^2}\frac{1}{\sqrt{2\pi}\sigma_\beta}e^{-\frac{1}{2}\left(\frac{\beta_i-\beta}{\sigma_\beta}\right)^2}dyd\beta_i
\end{aligned}$$

\newpage

# Crops continued

This question will deal with a data set of country-level statistics from [this source](https://www.gu.se/en/quality-government/qog-data/data-downloads/standard-dataset) with an explanation of the data encoding found [here](https://www.qogdata.pol.gu.se/data/codebook_std_jan22.pdf).  

A few useful columns:

- `mad_gdppc`: GDP per capita
- `wdi_araland`: Arable land (\% of land area)
- `wdi_precip`: Average annual precipitation (mm per year)
- `ht_region`: Country's region of the world: Eastern Europe (1), Latin America (2), North Africa & the Middle East (3), Sub-Saharan Africa (4), Western Europe and North America (5), East Asia (6), South-East Asia (7), South Asia (8), Pacific (9), Caribbean (10)

```{r, include=F}
countries <- read.csv("data/countries.csv")
```

1. Fit a quadratic regression model to predict `wdi_araland` from `wdi_precip`, using `log(mad_gdppc)` as a weight to account for the fact that wealthier countries might have more accurate crop tracking technology.  Call this `lm_weight`.

```{r}
lm_weight <- lm(wdi_araland~poly(wdi_precip, 2, raw = TRUE), countries, weight=log(mad_gdppc))
```

2. Use the `lmer` function to fit a mixed effects model with a random intercept based on the country's `ht_region`, and use the argument `weight=log(mad_gdppc)` here as well.  Call this `lmer1`.

```{r}
countries$ht_region <- as.factor(countries$ht_region)
lmer1 <- lmer(wdi_araland ~ poly(wdi_precip, 2, raw = TRUE) + (1 | ht_region), 
              countries, weight=log(mad_gdppc))
```
3. Use your models to plot the estimated relationship between precipitation and arable land.

```{r, warning=F, echo=F}
# Due to my loathing of base R plots, this chunk of code is devoted to creating dummy ranges 
# of precipitation values so that each model can show a curve

dummy_x <- seq(min(countries$wdi_precip, na.rm = T), max(countries$wdi_precip, na.rm=T), 1)
lm_unweighted <- lm(wdi_araland~poly(wdi_precip, 2, raw = TRUE), countries)
lm_unweighted_pred <- predict(lm_unweighted, newdata=data.frame(wdi_precip = dummy_x))
lm_weighted_pred <- predict(lm_weight, newdata=data.frame(wdi_precip = dummy_x))
countries_subset <- countries[!is.na(countries$wdi_precip) & 
                                !is.na(countries$wdi_araland) & 
                                !is.na(countries$mad_gdppc),]

# Trying to get the region weighting reasonable
lmer_pred <- predict(lmer1, newdata=data.frame(wdi_precip = rep(dummy_x, 9), 
                                               ht_region = rep(as.factor(c(1:8, 10)), each=length(dummy_x))
                                               ))
lmer_pred <- matrix(lmer_pred, ncol = 9)
lmer_pred <- as.vector(lmer_pred %*% as.matrix(unname(table(countries_subset$ht_region)/sum(table(countries_subset$ht_region)))[c(1:8, 10)]))

dummy_df <- data.frame(dummy_x, "Unweighted linear model" = lm_unweighted_pred, 
                       "Weighted linear model" = lm_weighted_pred, 
                       "Mixed effects model" = lmer_pred, check.names = F)
dummy_df <- melt(dummy_df, id.vars="dummy_x")
colnames(dummy_df) = c("dummy_x", "Model", "value")

ggplot(countries, aes(x=wdi_precip, y=wdi_araland)) + 
  geom_point() + 
  geom_line(data=dummy_df, aes(y=value, x=dummy_x, col=Model), size=0.5) + 
  xlab("Average annual precipitation (mm per year)") + 
  ylab("Arable land (% of land area)")
```
There's not much to interpret here: both of the weighted models have fitted curves slightly higher than the unweighted model (plausibly suggesting that wealthier countries might just have more arable land given the same amount of precipitation).  However, the curves are all quite similar and have maxima close to each other.

