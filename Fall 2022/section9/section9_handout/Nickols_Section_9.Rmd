---
output: pdf_document
header-includes:
   - \usepackage{tcolorbox}
   - \usepackage{fancyhdr}
   - \usepackage[utf8]{inputenc}
urlcolor: blue
---

\pagestyle{fancy}
\fancyhf{}
\rhead{Will Nickols}
\lhead{Section 9}
\rfoot{Page \thepage}

# Announcements
- Make sure to sign in on the [google form](https://forms.gle/JGvZP8CPUhaefnLT6)
- Pset 8 due November 11 at 5 pm
- Midterm 2 November 11 through November 18
- No section next week or the week after
- Last section on November 29.

# Decision Tree Algorithms

Consider a prediction problem where we want to predict $Y_i$ from a single $X_{i}$.  Recall the heuristic we used to describe how a decision tree decides where to split a node: consider many possible split points $x$ from $\min(\{X_{i}:i\in1,...n\})$ to $\max(\{X_{i}:i\in1,...n\})$ and calculate the SSE for each $$\sum_{i:X_{i}<x}(Y_i-\bar{Y}_{\textrm{left}})^2+\sum_{i:X_{i}\geq x}(Y_i-\bar{Y}_{\textrm{right}})^2$$ where $\bar{Y}_{\textrm{left}}$ is the mean of the $Y_i$ for $i$ such that $X_{i}<x$ (and analogously for $\bar{Y}_{\textrm{right}}$).  Then, choose the $x$ that minimizes this SSE.  This is a nice story, but the implementation is a bit more clever.

1. Consider an algorithm that did what was described above, and suppose that it divided the interval from $\min(\{X_{i}:i\in1,...n\})$ to $\max(X_{i}:i\in1,...n\})$ into $k$ equal segments.  Give a reason this would perform suboptimally when $n<k$ and a different reason it would perform suboptimally when $n>k$.  How many additions, subtractions, multiplications, and divisions (where each operation only involves two numbers) would be required to choose the best split?  (Note that becuase we're only concerned with the algorithm here, we'll treat both the $X_i$ and $Y_i$ as fixed.)

2. A better idea would be to only consider our observed $X_{i}$ (or the average between consecutive $X_{i}$) as potential split points.  How many additions, subtractions, multiplications, and divisions would be required to choose the best split now?  (Assume that at least one data point is in the "left" group and at least one is in the "right" group.)

3. We can still do better!  Let $L_s$ (left sum) be the sum of the $Y_i$ for the smallest $s$ $X_{i}$, and let $R_s$ (right sum) be the sum of the $Y_i$ for the rest of the data points.  Let $L'_s$ and $R_s'$ be the corresponding means.  Show that the $\textrm{SSE}$ can be written in terms of a constant (a piece that doesn't change with $s$) and a piece that only depends on $L'_s$, $R_s'$, and $s$ (and the constants $n$ and $\bar{Y}$).  Next, find a simple equation for $L_{s+1}$, $R_{s+1}$, $L_{s+1}'$, and $R_{s+1}'$ in terms of $L_{s}$ and $R_{s}$.  Finally, determine how many additions, subtractions, multiplications, and divisions would be required to choose the best split.  Hint for the first part: Use (twice) the fact that 
$$\sum_{i=1}^n(Y_i - \bar Y)^2 = \sum_{i=1}^n(Y_i-c)^2 - n(c-\bar Y)^2$$

# Categorical decision trees

In this class, we mainly look at decision trees as a non-parametric way of making a prediction about a continuous variable of interest.  However, another (probably more common) use of decision trees is for predicting categorical variables.  While looking at the sum of squares error is a reasonable way to build decision trees for continuous predictions, other methods can be more useful for categorical predictions.  This question will look at two such options.  Throughout this problem, imagine that we have a data set $\mathbf{X}$ ($n\times p$) and a set of true categories $\vec{Y}$ ($n\times 1$) with $k$ possible values ($k$ categories).

1. A first method involves calculating the entropy of the parent and child nodes.  Entropy is defined as:
$$E=-\sum_{i=1}^kp_i\log_2p_i$$
where $p_i$ is the proportion of the items in the node from class $i$.  (If $p_i=0$, we treat its $p_i\log_2p_i$ term as $0$ since $p_i\log_2p_i\rightarrow0$ as $p_i\rightarrow0$.)  When splitting using this metric, the parent node's entropy $E_{\textrm{Parent}}$ is calculated, and the weighted average of the children's entropies are calculated ($\frac{1}{n_{\textrm{Left}}}E_{\textrm{Left}}+\frac{1}{n_{\textrm{Right}}}E_{\textrm{Right}}$).  The split that yields the lowest entropy is chosen.  Show that the entropy of a node is minimized when the node is "pure" (there are only items of one class in the node), and show (for $k=2$) that the entropy of the node is maximized when there are an equal proportion of items from each class in the node.  (This second statement can be extended to show that the entropy is maximized when the distribution of classes in a node is uniform.)

2. Because calculating $\log$s can be computationally expensive, a more common method is to maximize the Gini value: $$G=\sum_{i=1}^kp_i^2$$ where $p_i$ is the proportion of the items in the node from class $i$.  Explain why the Gini value can be interpreted as the probability that a randomly chosen item in the node would be assigned its correct class when assigning classes randomly according to the proportions in the node.  Also, find when the Gini value is maximized and find when it is minimized for $k=2$.

3. For a 2-class classification problem, plot the Entropy and Gini values on the same graph.

```{r, warning=F}
# TODO: Plot Entropy and Gini
```

# The Entire Arboretum

The packages `rpart` and `randomForest` will be the main workhorses of our decision trees and random forests.  The most important functions for us will be:

- `rpart(formula, data, control)` where `control` is something like `list(minsplit=1,cp=0,maxdepth=20)`, where `minsplit` says we want the node to be split if it has at least 1 observation; `cp` (complexity parameter) says we will accept any improvement in fit; and `maxdepth` is that maximum node depth.
- `prune(tree, cp)` where `cp` is the complexity parameter.
- `randomForest(formula, data, maxnodes, mtry, ntree)` where `mtry` is the number of variables randomly sampled as candidates at each split, and `ntree` is the number of trees to grow.

This question will deal with a data set of country-level statistics from [this source](https://www.gu.se/en/quality-government/qog-data/data-downloads/standard-dataset) with an explanation of the data encoding found [here](https://www.qogdata.pol.gu.se/data/codebook_std_jan22.pdf).  

A few useful columns:

- `spi_ospi`: Overall social progress index on 0-100 scale
- `mad_gdppc`: GDP per capita
- `wdi_internet`: Percent of population using the internet
- `wdi_birth`: Birth rate per 1000 people
- `wdi_chexppgdp`: Current health expenditures as percent of GDP
- `wdi_elerenew`: Percent of total electricity output that's renewable
- `wdi_lifexp`: Life expectancy at birth
- `wdi_wip`: Proportion of seats held by women in national parliaments
- `wdi_popurb`: Percentage of total population that is urban
- `wdi_imig`: Proportion of people born outside the country in which they live

```{r, include=F}
library(randomForest)
RMSE = function(y,yhat){
  SSE = sum((y-yhat)^2)
  return(sqrt(SSE/length(y)))  
}

library(rpart)
library(rpart.plot)
set.seed(139)

countries <- read.csv("data/countries.csv")

# All coefficients of interest
lm1 <- lm(spi_ospi ~ mad_gdppc + wdi_internet + wdi_birth + wdi_chexppgdp + 
                        wdi_elerenew + wdi_lifexp + wdi_wip + wdi_popurb + wdi_imig, countries)

summary(lm1)
```

1. Fit a complex regression tree to predict `spi_ospi` from the above predictors, plot the decision tree, and report the RMSE and $R^2$.  Use `minsplit=1, cp=0, maxdepth=20` in `rpart`.

```{r, warning=F}
# TODO: Fit basic tree

# TODO: Plot tree

# TODO: Calculate RMSE

# TODO: Calculate Rsq
```

2. Fit a well-pruned regression tree to predict `spi_ospi` from the above predictors, plot the tree, and report the RMSE and $R^2$.  Start with `minsplit=1, cp=0, maxdepth=20)` in `rpart` and use the default 10-fold cross-validation to prune based on `cp`.  Note: `rpart` automatically performs 10-fold CV, and the best `cp` can be determined from a tree with the command `tree1$cptable[,"CP"][which.min(tree1$cptable[,"xerror"])]`

```{r}
# TODO: Prune tree

# TODO: Plot tree

# TODO: RMSE

# TODO: Rsq
```

3. Fit a well-tuned random forest model to predict `spi_ospi` from the above predictors considering `mtry = c(2,3,4)` and `maxnodes = c(3,5,10)` with `ntree=200`. Report RMSE and $R^2$ as well as the best `mtry` and best `maxnodes`.  

```{r, warning=F}
param_grid = expand.grid(mtries = c(2, 3, 4), maxnodes = c(3, 5, 10))

curRMSE <- Inf

bestmtry <- 0
bestnode <- 0

# Random forest doesn't handle missing data well so keep only observations with no NAs
eval_subset = countries[rowSums(is.na(countries[,c("spi_ospi", "mad_gdppc", 
                                                   "wdi_internet", "wdi_birth", 
                                                   "wdi_chexppgdp", "wdi_elerenew", 
                                                   "wdi_lifexp", "wdi_wip", 
                                                   "wdi_popurb", "wdi_imig")]))==0,]

for (i in nrow(param_grid)) {
  # TODO: Select mtry and maxnode from the parameter grid
  
  # TODO: Fit random forest
  
  # TODO: If RMSE is the best so far, store the model
}

# TODO: RMSE

# TODO: Rsq

# TODO: Best mtry and maxnode
```

4. Interpret the relationship of `spi_ospi` with `wdi_lifexp` in the first tree and the best random forest model through predictions on a single plot.  Describe what you see in a few sentences.

```{r, eval=F}
# Copy original data 
dummy_df = eval_subset

dummylifexp = # TODO: Make dummy life expectancy vector

# Show original points
plot(spi_ospi ~ wdi_lifexp, data=countries, col=rgb(0.5,0.5,0.5,0.5), cex=0.5, pch=16)

# Get predictions on the dummy data
yhats=matrix(NA,nrow=nrow(dummy_df),ncol=length(dummylifexp))
yhats2=matrix(NA,nrow=nrow(dummy_df),ncol=length(dummylifexp))
for(i in 1:nrow(dummy_df)){
  # For each original data points, test all dummy life expectancies
  rows=dummy_df[rep(i,length(dummylifexp)),]
  rows$wdi_lifexp=dummylifexp
  yhat = predict(tree1,new=rows)
  yhats[i,]=yhat
  yhat2 = predict(rfsave, new=rows)
  yhats2[i,]=yhat2
}

# Take the mean over the predicted OSPI at each dummy life expectancy
mean_yhat = apply(yhats,2,mean)
mean_yhat2 = apply(yhats2, 2, mean)

# Tree in brown
lines(mean_yhat~dummylifexp,col=rgb(0.5,0.25,0,1),lwd=3)

# Forest in green
lines(mean_yhat2~dummylifexp,col=rgb(0,0.5,0,1),lwd=3)
```

5. Which predictors are most important in the first tree and the best random forest?  How do they compare in relative importance?  

```{r}
# TODO: Plot variable importance
```




