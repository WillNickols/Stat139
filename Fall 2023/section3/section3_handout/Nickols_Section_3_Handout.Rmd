---
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage{tcolorbox}
- \usepackage{fancyhdr}
- \usepackage[utf8]{inputenc}
- \usepackage{wrapfig}
- \usepackage{amsmath}
- \usepackage{booktabs}
- \usepackage{esvect}
urlcolor: blue
---

\pagestyle{fancy}
\fancyhf{}
\rhead{Will Nickols}
\lhead{Section 3}
\rfoot{Page \thepage}

# Announcements

\begin{wrapfigure}{r}{0.12\textwidth}
  \centering
    \vspace*{-1.3cm}
    \includegraphics[width=\linewidth]{section_qr_code.png}
\end{wrapfigure}

Make sure to sign in on the [google form](https://forms.gle/xm1DfzuZFNcWU6fH8) (I send a list of which section questions are useful for which pset questions afterwards)

Pset 2 due Saturday 9/30

# Introductions (again)
- Name
- One question or thought related to lecture last week (ranks, bootstrap, randomization)

```{r, echo=F, warning=F, message=F, cache=F}
list.of.packages <- c("ggplot2", "gridExtra")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

library(ggplot2)
library(gridExtra)
library(dplyr)
```

# Hypothesis testing on real data

These problems will deal with a dataset of country-level statistics from [UNdata](https://data.un.org/) and [Varieties of Democracy](https://v-dem.net/data/the-v-dem-dataset/).

```{r, echo=F}
# Read in the data
countries <- read.csv("data/country_stats.csv", check.names = F)
```

1. Suppose we want to test for a difference in mean 2010 GDP per capita between democracies and non-democracies. The following plots show the distributions. Which tests would be valid?

```{r, echo=F, fig.width=5, fig.height=3, fig.align='center', cache=T, warning=F}
countries$dem = countries$v2x_polyarchy > 0.5
tmp_countries <- countries[countries$Year == 2010 & !is.na(countries$dem),]

ggplot(tmp_countries, aes(x=`GDP per capita (US dollars)`, fill=dem)) + 
  geom_histogram(alpha=0.4, position="identity", bins = 30) + 
  xlab("GDP per capita") + 
  ylab("Countries") + 
  theme_bw() + 
  labs(fill="Democracy")
```

\vspace{2 cm}

2. Perform a formal rank-sum test for the difference in GDP per capita between democracies and non-democracies.

```{r, echo=F}
dem_gdps = countries$`GDP per capita (US dollars)`[countries$Year == 2010 & !is.na(countries$dem) & countries$dem]
nondem_gdps = countries$`GDP per capita (US dollars)`[countries$Year == 2010 & !is.na(countries$dem) & !(countries$dem)]

wilcox.test(dem_gdps, nondem_gdps)
```

\vspace{2 cm}

3. Perform a formal log-transformed $t$-test for the difference in GDP per capita between democracies and non-democracies. Give a 95% confidence interval for the ratio of medians.

```{r, echo=F}
t.test(log(dem_gdps), log(nondem_gdps))
```

\vspace{2 cm}

\newpage

# Variance by decomposition

Let $X\sim\textrm{Bin}(n,p)$ and $Y\sim\textrm{Bin}(m,p)$. Let $X+Y=r$.

1. Find the variance of $X|r$ by using the variance of a known distribution (See 3.9.2 in the Stat 110 book for a hint).

\vspace{3 cm}

2. Find the variance of $X|r$ by using the fact that $\textrm{Var}(X+Y|r)=0$ and treating $X$ and $Y$ as the sum of Bernoulli random variables. Verify that the two answers are the same. (Hint: Once you get to the Bernoulli random variables, think about how knowing the sum is $r$ makes $p$ irrelevant.)

\vspace{10 cm}

\newpage

# Everything everywhere all at once (all two-sample continuous comparisons)

Let $X_1,\dots X_{n_1}\sim \textrm{Exp}(1/\mu_1)$ and $Y_1,\dots Y_{n_2}\sim \textrm{Exp}(1/\mu_2)$.

1. Name three tests we've learned so far that would not be applicable for comparing the $X$s and $Y$s.

\vspace{2 cm}

2. We'll be comparing the Type I and II error for the following tests: a two sample $t$ test, a log-transformed $t$-test, a rank-based test, and a permutation test. We'll consider two scenarios:
- First, $n_1=5$, $n_2=15$ with $\mu_1=\mu_2=5$ when calculating the Type I error rate and $\mu_1=5$ and $\mu_2=3$ when calculating the Type II error rate.
- Second, the same as before but with $n_1=20$.

Why should we use $\mu_1=\mu_2$ when calculating the Type I error rate but $\mu_1\neq\mu_2$ when calculating the Type II error rate?

\vspace{2 cm}

3. Compare the results. What has the highest power? Which maintain their nominal false positive rates? Which test is best in which situations?

```{r, cache=T, echo=F}
set.seed(139)
nsims = 5000

run_sim <- function(mu_1, mu_2, n, m) {
  nboot = 250
  
  t_test_out = vector(length = nsims)
  transformed_t_out = vector(length = nsims)
  rank_out = vector(length = nsims)
  perm_test_out = vector(length = nsims)

  for (i in 1:nsims) {
    x = rexp(n, 1/mu_1)
    y = rexp(m, 1/mu_2)
    t_test_out[i] = t.test(x, y)$p.value
    transformed_t_out[i] = t.test(log(x), log(y))$p.value
    rank_out[i] = wilcox.test(x, y)$p.value
    df = cbind(c(x,y), c(rep(0, n), rep(1, m)))
    boot_diff = vector(length = nboot)
    for (j in 1:nboot) {
      df_tmp = cbind(df[,1], sample(df[,2], n+m))
      boot_diff[j] = mean(df_tmp[df_tmp[,2]==0,1]) - mean(df_tmp[df_tmp[,2]==1,1])
    }
    perm_test_out[i] = mean(abs(boot_diff) >= abs(mean(x) - mean(y)))
  }
  
  output = data.frame("test" = c("t test", "log t test", "rank test", "perm test"),
                 "diff" = c(mean(t_test_out <= 0.05), mean(transformed_t_out <= 0.05), 
                               mean(rank_out <= 0.05), mean(perm_test_out <= 0.05)),
                 "not_diff" = c(mean(t_test_out > 0.05), mean(transformed_t_out > 0.05), 
                               mean(rank_out > 0.05), mean(perm_test_out > 0.05)))
  
  return(output)
}

output_1 <- run_sim(5, 3, 5, 15)
output_2 <- run_sim(5, 5, 5, 15)

out_df <- data.frame("Test (First scenario)" = c("t test", "log t test", "Rank test", "Permutation test"),
           "Type I" = round(output_2$diff, 3),
           "Type II" = round(output_1$not_diff, 3), check.names = F)

knitr::kable(out_df)

output_1 <- run_sim(5, 3, 20, 15)
output_2 <- run_sim(5, 5, 20, 15)

out_df <- data.frame("Test (Second scenario)" = c("t test", "log t test", "Rank test", "Permutation test"),
           "Type I" = round(output_2$diff, 3),
           "Type II" = round(output_1$not_diff, 3), check.names = F)

knitr::kable(out_df)
```

\vspace{5 cm}

4. What assumptions do we need for each test and what hypotheses are we testing?

\vspace{8 cm}
  
5. The following simulation uses the same set-ups as above to calculate a $t$-based confidence interval for the difference in means, a $t$-based confidence interval for the ratio of medians, a percentile bootstrap interval for the difference in means, and a reversed percentile bootstrap interval for the difference in means. Shown below are the coverage probability and interval width for each. Comment on the results.

```{r, cache=T, echo=F}
set.seed(139)
nsims = 5000

run_sim2 <- function(mu_1, mu_2, n, m) {
  nboot = 250
  
  t_cap = vector(length = nsims)
  t_length = vector(length = nsims)
  t_med_cap = vector(length = nsims)
  t_med_length = vector(length = nsims)
  perc_boot_cap = vector(length = nsims)
  perc_boot_length = vector(length = nsims)
  rev_boot_cap = vector(length = nsims)
  rev_boot_length = vector(length = nsims)
  
  df = min(n-1, m-1)
  
  for (i in 1:nsims) {
    x = rexp(n, 1/mu_1)
    y = rexp(m, 1/mu_2)
    
    # T-based interval
    t_out = t.test(x, y)$conf.int
    t_cap[i] = mu_1-mu_2 >= t_out[1] & mu_1-mu_2 <= t_out[2]
    t_length[i] = t_out[2] - t_out[1]
    
    # Transformed t interval
    t_med_out = t.test(log(x), log(y))$conf.int
    t_med_cap[i] = qexp(0.5, 1/mu_1)/qexp(0.5, 1/mu_2) >= exp(t_med_out[1]) & 
      qexp(0.5, 1/mu_1)/qexp(0.5, 1/mu_2) <= exp(t_med_out[2])
    t_med_length[i] = exp(t_med_out[2]) - exp(t_med_out[1])
    
    # Bootstrap
    boot_diff = vector(length = nboot)
    for (j in 1:nboot) {
      x_star = x[sample(1:n, n, replace = T)]
      y_star = y[sample(1:m, m, replace = T)]
      boot_diff[j] = mean(x_star) - mean(y_star)
    }
    
    perc_boot_cap[i] = mu_1-mu_2 >= quantile(boot_diff, 0.025) & 
      mu_1-mu_2 <= quantile(boot_diff, 0.975)
    perc_boot_length[i] = quantile(boot_diff, 0.975) - quantile(boot_diff, 0.025)
    
    bounds = mean(x)-mean(y) - 
      (quantile(boot_diff, c(0.025, 0.975)) - (mean(x)-mean(y)))
    
    rev_boot_cap[i] = mu_1-mu_2 >= bounds[2] & mu_1-mu_2 <= bounds[1]
    rev_boot_length[i] = bounds[1] - bounds[2]
  }
  
  output = data.frame("name" = c("t interval", "log t interval",
                                       "percentile boot", "reverse boot"),
                            "capture proportion" = c(mean(t_cap), mean(t_med_cap), 
                               mean(perc_boot_cap), mean(rev_boot_cap)),
                            "length" = c(mean(t_length), mean(t_med_length), 
                               mean(perc_boot_length), mean(rev_boot_length)),
                      check.names = F)
  
  return(output)
}

output_1 <- run_sim2(5, 3, 5, 15)
output_2 <- run_sim2(5, 5, 5, 15)

out_df <- data.frame("Interval (First scenario)" = c("t interval", "Transformed t interval", "Percentile bootstrap", "Rev. Perc. bootstrap"),
           "Coverage probability (means different)" = round(output_1$`capture proportion`, 3),
           "Interval width (means different)" = round(output_1$length, 2),
           "Coverage probability (means same)" = round(output_2$`capture proportion`, 3),
           "Interval width (means same)" = round(output_2$length, 2),
           check.names = F)

knitr::kable(out_df)

output_3 <- run_sim2(5, 3, 20, 15)
output_4 <- run_sim2(5, 5, 20, 15)

out_df2 <- data.frame("Interval (Second scenario)" = c("t interval", "Transformed t interval", "Percentile bootstrap", "Rev. Perc. bootstrap"),
           "Coverage probability (means different)" = round(output_3$`capture proportion`, 3),
           "Interval width (means different)" = round(output_3$length, 2),
           "Coverage probability (means same)" = round(output_4$`capture proportion`, 3),
           "Interval width (means same)" = round(output_4$length, 2),
           check.names = F)

knitr::kable(out_df2)
```

\vspace{5 cm}

6. Based on the results above, which is the best confidence interval to use?

\vspace{3 cm}

7. Imagine now that we wanted to construct a confidence interval for $\mu_1$ by using a studentized bootstrap interval.  If we knew the data were distributed exponentially, what's one small change we could make to the confidence interval for $\mu_1$ so that the interval is equally as wide or narrower while keeping the same confidence level?

\vspace{2 cm}








