---
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage{tcolorbox}
- \usepackage{fancyhdr}
- \usepackage[utf8]{inputenc}
- \usepackage{wrapfig}
- \usepackage{amsmath}
- \usepackage{booktabs}
- \usepackage{esvect}
urlcolor: blue
---

\pagestyle{fancy}
\fancyhf{}
\rhead{Will Nickols}
\lhead{Section 3}
\rfoot{Page \thepage}

# Announcements

\begin{wrapfigure}{r}{0.12\textwidth}
  \centering
    \vspace*{-1.3cm}
    \includegraphics[width=\linewidth]{section_qr_code.png}
\end{wrapfigure}

Make sure to sign in on the [google form](https://forms.gle/xm1DfzuZFNcWU6fH8) (I send a list of which section questions are useful for which pset questions afterwards)

Pset 2 due Friday 9/29

# Introductions (again)
- Name
- One question or thought related to lecture last week (ranks, bootstrap, randomization)

```{r, echo=F, warning=F, message=F, cache=F}
list.of.packages <- c("ggplot2", "gridExtra")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

library(ggplot2)
library(gridExtra)
library(dplyr)
```

# Hypothesis testing on real data

These problems will deal with a dataset of country-level statistics from [UNdata](https://data.un.org/) and [Varieties of Democracy](https://v-dem.net/data/the-v-dem-dataset/).

```{r, echo=F}
# Read in the data
countries <- read.csv("data/country_stats.csv", check.names = F)
```

1. Suppose we want to test for a difference in mean 2010 GDP per capita between democracies and non-democracies. The following plots show the distributions. Which tests would be valid?

```{r, echo=F, fig.width=5, fig.height=3, fig.align='center', cache=T, warning=F}
countries$dem = countries$v2x_polyarchy > 0.5
tmp_countries <- countries[countries$Year == 2010 & !is.na(countries$dem),]

ggplot(tmp_countries, aes(x=`GDP per capita (US dollars)`, fill=dem)) + 
  geom_histogram(alpha=0.4, position="identity", bins = 30) + 
  xlab("GDP per capita") + 
  ylab("Countries") + 
  theme_bw() + 
  labs(fill="Democracy")
```

An unpaired $t$-test, a rank-sum test, or a log-transformed $t$-test would all be reasonable. The rank-sum test and log-transformed $t$-test would account for the fact that the data is not Normally distributed. However, we have enough data points that the sample means will be approximately normally distributed, so an unpaired $t$-test could work as well.

2. Perform a formal rank-sum test for the difference in GDP per capita between democracies and non-democracies.

```{r, echo=F}
dem_gdps = countries$`GDP per capita (US dollars)`[countries$Year == 2010 & !is.na(countries$dem) & countries$dem]
nondem_gdps = countries$`GDP per capita (US dollars)`[countries$Year == 2010 & !is.na(countries$dem) & !(countries$dem)]

wilcox.test(dem_gdps, nondem_gdps)
```
Our hypotheses are $H_0:$ the distributions of 2010 GDP per capita are the same between democracies and non-democracies versus $H_a:$ they are different. We get a test statistic of $W=5443$ and a p-value of $7.75\times 10^{-8}$, so we reject the null and conclude that democracies tend to have higher GDPs per capita.

3. Perform a formal log-transformed $t$-test for the difference in GDP per capita between democracies and non-democracies. Give a 95% confidence interval for the ratio of medians.

```{r, echo=F}
t.test(log(dem_gdps), log(nondem_gdps))
```

Let $\mu_0$ be the mean log GDP per capita of non-democracies and $\mu_1$ be the mean log GDP per capita of democracies. We are testing $H_0$: $\mu_0=\mu_1$ vs $H_a:$ the two means are different. We get a $t$-statistic of $-5.85$ with $169.6$ degrees of freedom, which corresponds to a p-value of $2.5\times10^{-8}$. Therefore, we reject the null and conclude that democratic countries have higher average log GDP per capita. To find a 95% confidence interval for the ratio of medians, we can exponentiate the current interval: $(\exp(0.82), \exp(1.66))=(2.27,5.24)$.

\newpage

# Variance by decomposition

Let $X\sim\textrm{Bin}(n,p)$ and $Y\sim\textrm{Bin}(m,p)$. Let $X+Y=r$.

1. Find the variance of $X|r$ by using the variance of a known distribution (See 3.9.2 in the Stat 110 book for a hint).

$X|r\sim \textrm{HGeom}(n,m,r)$, so by the variance of the hypergeometric we have:
$$\textrm{Var}(X|r)=\frac{n+m-r}{n+m-1}\cdot\frac{nr}{n+m}\cdot\frac{m}{n+m}$$

2. Find the variance of $X|r$ by using the fact that $\textrm{Var}(X+Y|r)=0$ and treating $X$ and $Y$ as the sum of Bernoulli random variables. Verify that the two answers are the same. (Hint: Once you get to the Bernoulli random variables, think about how knowing the sum is $r$ makes $p$ irrelevant.)

First, note that the variance of a constant is 0, so $\textrm{Var}(X+Y|r)=0$. Each of $X$ and $Y$ can be decomposed into Bernoullis, and each of these have the same variance and covariance by symmetry. Therefore, $$0=\textrm{Var}(X+Y|r)=(n+m)\textrm{Var}(I_1|r)+2{n+m\choose 2}\textrm{Cov}(I_1,I_2|r)$$ Then, we can solve for the covariance: $\textrm{Cov}(I_1,I_2|r)=-\textrm{Var}(I_1|r)/(n+m-1)$. Conditioning on $r$, $P(I_1=1|r)=\frac{r}{n+m}$, so $$\textrm{Var}(I_1|r)=\frac{r}{n+m}\cdot\frac{n+m-r}{n+m}$$ and $$\textrm{Cov}(I_1,I_2|r)=-\frac{r}{n+m}\cdot\frac{n+m-r}{(n+m)(n+m-1)}$$
Then, building up $X$ again, 
$$\begin{aligned}
\textrm{Var}(X|r)&=\textrm{Var}(\sum_{i=1}^nI_i)\\
&=n\cdot\frac{r}{n+m}\cdot\frac{n+m-r}{n+m}-2{n\choose 2}\frac{r}{n+m}\cdot\frac{n+m-r}{(n+m-1)(n+m)}\\
&=\frac{nr(n+m-r)}{(n+m)^2}\left[1-\frac{n-1}{n+m-1}\right]\\
&=\frac{nmr(n+m-r)}{(n+m)^2(n+m-1)}
\end{aligned}$$

\newpage

# Everything everywhere all at once (all two-sample continuous comparisons)

Let $X_1,\dots X_{n_1}\sim \textrm{Exp}(1/\mu_1)$ and $Y_1,\dots Y_{n_2}\sim \textrm{Exp}(1/\mu_2)$.

1. Name three tests we've learned so far that would not be applicable for comparing the $X$s and $Y$s.

ANOVA (only two groups), paired $t$-test (different number of observations in each group), proportion test (not proportions)

2. We'll be comparing the Type I and II error for the following tests: a one sample $t$ test, a log-transformed $t$-test, a rank-based test, and a permutation test. We'll consider two scenarios:
- First, $n_1=5$, $n_2=15$ with $\mu_1=\mu_2=5$ when calculating the Type I error rate and $\mu_1=5$ and $\mu_2=3$ when calculating the Type II error rate.
- Second, the same as before but with $n_1=20$.

Why should we use $\mu_1=\mu_2$ when calculating the Type I error rate but $\mu_1\neq\mu_2$ when calculating the Type II error rate?

A type I error is when we reject the null but we should have retained it. We should retain the null when $\mu_1$ and $\mu_2$ are indeed equal. A type II error is when we fail to reject the null when we should have rejected it. Therefore, the means must be different to make this error.

3. Compare the results. What has the highest power? Which maintain their nominal false positive rates? Which test is best in which situations?

```{r, cache=T, echo=F}
set.seed(139)
nsims = 5000

run_sim <- function(mu_1, mu_2, n, m) {
  nboot = 250
  
  t_test_out = vector(length = nsims)
  transformed_t_out = vector(length = nsims)
  rank_out = vector(length = nsims)
  perm_test_out = vector(length = nsims)

  for (i in 1:nsims) {
    x = rexp(n, 1/mu_1)
    y = rexp(m, 1/mu_2)
    t_test_out[i] = t.test(x, y)$p.value
    transformed_t_out[i] = t.test(log(x), log(y))$p.value
    rank_out[i] = wilcox.test(x, y)$p.value
    df = cbind(c(x,y), c(rep(0, n), rep(1, m)))
    boot_diff = vector(length = nboot)
    for (j in 1:nboot) {
      df_tmp = cbind(df[,1], sample(df[,2], n+m))
      boot_diff[j] = mean(df_tmp[df_tmp[,2]==0,1]) - mean(df_tmp[df_tmp[,2]==1,1])
    }
    perm_test_out[i] = mean(abs(boot_diff) >= abs(mean(x) - mean(y)))
  }
  
  output = data.frame("test" = c("t test", "log t test", "rank test", "perm test"),
                 "diff" = c(mean(t_test_out <= 0.05), mean(transformed_t_out <= 0.05), 
                               mean(rank_out <= 0.05), mean(perm_test_out <= 0.05)),
                 "not_diff" = c(mean(t_test_out > 0.05), mean(transformed_t_out > 0.05), 
                               mean(rank_out > 0.05), mean(perm_test_out > 0.05)))
  
  return(output)
}

output_1 <- run_sim(5, 3, 5, 15)
output_2 <- run_sim(5, 5, 5, 15)

out_df <- data.frame("Test (First scenario)" = c("t test", "log t test", "Rank test", "Permutation test"),
           "Type I" = round(output_2$diff, 3),
           "Type II" = round(output_1$not_diff, 3), check.names = F)

knitr::kable(out_df)

output_1 <- run_sim(5, 3, 20, 15)
output_2 <- run_sim(5, 5, 20, 15)

out_df <- data.frame("Test (Second scenario)" = c("t test", "log t test", "Rank test", "Permutation test"),
           "Type I" = round(output_2$diff, 3),
           "Type II" = round(output_1$not_diff, 3), check.names = F)

knitr::kable(out_df)
```

In the first scenario where we have a very small sample size, the $t$-test is slightly above its nominal Type I error rate (0.05), and its type II error is the highest in the group (which means it has the lowest power). However, its assumptions are severely violated by using a very right-skewed distribution with only 5 data points. The other tests are closer to their nominal Type I error rates (0.05) with the permutation test having the highest power.

In the second scenario where we have reasonable sample sizes, all the tests have about the same Type I error rate (near 0.05), but the $t$ test has the highest power. The takeaway is that even with skewed distributions, as long as the sample size is at least moderate, the $t$ test will have the highest power while maintaining its nominal Type I error rate.

4. What assumptions do we need for each test and what hypotheses are we testing?

- $t$-test
  - Assumptions: independence within and between groups and normality (or a large sample size)
  - Hypotheses: $H_0:$ Means are equal; $H_a$: Means are not equal.
- Log-transformed $t$-test
  - Assumptions: independence, symmetry once transformed, and normality once transformed (or a large sample size)
  - Hypotheses: $H_0:$ Ratio of medians is 1; $H_a:$ Ratio of medians isn't 1.
- Rank-based test
  - Assumptions: independence; $n_1,n_2\geq10$
  - Hypotheses: $H_0:$ The two distributions are the same, $H_a$: The two distributions are different.
- Permutation test
  - Assumptions: independence
  - Hypotheses: $H_0:$ The distribution of outcomes is not related to group status. $H_a:$ It is related.
  
5. The following simulation uses the same set-ups as above to calculate a $t$-based confidence interval for the difference in means, a $t$-based confidence interval for the ratio of medians, a percentile bootstrap interval for the difference in means, and a reversed percentile bootstrap interval for the difference in means. Shown below are the coverage probability and interval width for each. Comment on the results.

```{r, cache=T, echo=F}
set.seed(139)
nsims = 5000

run_sim2 <- function(mu_1, mu_2, n, m) {
  nboot = 250
  
  t_cap = vector(length = nsims)
  t_length = vector(length = nsims)
  t_med_cap = vector(length = nsims)
  t_med_length = vector(length = nsims)
  perc_boot_cap = vector(length = nsims)
  perc_boot_length = vector(length = nsims)
  rev_boot_cap = vector(length = nsims)
  rev_boot_length = vector(length = nsims)
  
  df = min(n-1, m-1)
  
  for (i in 1:nsims) {
    x = rexp(n, 1/mu_1)
    y = rexp(m, 1/mu_2)
    
    # T-based interval
    t_out = t.test(x, y)$conf.int
    t_cap[i] = mu_1-mu_2 >= t_out[1] & mu_1-mu_2 <= t_out[2]
    t_length[i] = t_out[2] - t_out[1]
    
    # Transformed t interval
    t_med_out = t.test(log(x), log(y))$conf.int
    t_med_cap[i] = qexp(0.5, 1/mu_1)/qexp(0.5, 1/mu_2) >= exp(t_med_out[1]) & 
      qexp(0.5, 1/mu_1)/qexp(0.5, 1/mu_2) <= exp(t_med_out[2])
    t_med_length[i] = exp(t_med_out[2]) - exp(t_med_out[1])
    
    # Bootstrap
    boot_diff = vector(length = nboot)
    for (j in 1:nboot) {
      x_star = x[sample(1:n, n, replace = T)]
      y_star = y[sample(1:m, m, replace = T)]
      boot_diff[j] = mean(x_star) - mean(y_star)
    }
    
    perc_boot_cap[i] = mu_1-mu_2 >= quantile(boot_diff, 0.025) & 
      mu_1-mu_2 <= quantile(boot_diff, 0.975)
    perc_boot_length[i] = quantile(boot_diff, 0.975) - quantile(boot_diff, 0.025)
    
    bounds = mean(x)-mean(y) - 
      (quantile(boot_diff, c(0.025, 0.975)) - (mean(x)-mean(y)))
    
    rev_boot_cap[i] = mu_1-mu_2 >= bounds[2] & mu_1-mu_2 <= bounds[1]
    rev_boot_length[i] = bounds[1] - bounds[2]
  }
  
  output = data.frame("name" = c("t interval", "log t interval",
                                       "percentile boot", "reverse boot"),
                            "capture proportion" = c(mean(t_cap), mean(t_med_cap), 
                               mean(perc_boot_cap), mean(rev_boot_cap)),
                            "length" = c(mean(t_length), mean(t_med_length), 
                               mean(perc_boot_length), mean(rev_boot_length)),
                      check.names = F)
  
  return(output)
}

output_1 <- run_sim2(5, 3, 5, 15)
output_2 <- run_sim2(5, 5, 5, 15)

out_df <- data.frame("Interval (First scenario)" = c("t interval", "Transformed t interval", "Percentile bootstrap", "Rev. Perc. bootstrap"),
           "Coverage probability (means different)" = round(output_1$`capture proportion`, 3),
           "Interval width (means different)" = round(output_1$length, 2),
           "Coverage probability (means same)" = round(output_2$`capture proportion`, 3),
           "Interval width (means same)" = round(output_2$length, 2),
           check.names = F)

knitr::kable(out_df)

output_3 <- run_sim2(5, 3, 20, 15)
output_4 <- run_sim2(5, 5, 20, 15)

out_df2 <- data.frame("Interval (Second scenario)" = c("t interval", "Transformed t interval", "Percentile bootstrap", "Rev. Perc. bootstrap"),
           "Coverage probability (means different)" = round(output_3$`capture proportion`, 3),
           "Interval width (means different)" = round(output_3$length, 2),
           "Coverage probability (means same)" = round(output_4$`capture proportion`, 3),
           "Interval width (means same)" = round(output_4$length, 2),
           check.names = F)

knitr::kable(out_df2)
```

When one of the samples is very small and there's a difference, all of the intervals except the interval for the ratio of medians show coverage probabilities quite a bit below their nominal levels. Interestingly, the percentile methods are even worse than the t interval. When the means are the same, all the methods perform a bit better. The interval widths correlate inversely with the coverage probability: smaller intervals have lower coverage probabilities. 

When the samples are larger, the coverage probabilities are closer to their nominal levels, but the percentile methods still give intervals that are too narrow and therefore have slightly lower coverage probabilities. Note that in both scenarios the transformed $t$ interval is trying to capture something different than the other intervals, explaining its lower width.

6. Based on the results above, which is the best confidence interval to use?

The $t$ or transformed $t$ have coverage probabilities closest to the nominal confidence level, so we should use those. Shorter intervals would be nice if the intervals maintained the same coverage probabilities, but otherwise they're just not calibrated and therefore not useful.

7. Imagine now that we wanted to construct a confidence interval for $\mu_1$ by using a studentized bootstrap interval.  If we knew the data were distributed exponentially, what's one small change we could make to the confidence interval for $\mu_1$ so that the interval is equally as wide or narrower while keeping the same confidence level?

Make the lower bound 0 if it's ever negative in the studentized bootstrap interval.

\newpage
















