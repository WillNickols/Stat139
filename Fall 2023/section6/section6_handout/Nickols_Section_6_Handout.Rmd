---
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage{tcolorbox}
- \usepackage{fancyhdr}
- \usepackage[utf8]{inputenc}
- \usepackage{wrapfig}
- \usepackage{amsmath}
- \usepackage{booktabs}
- \usepackage{esvect}
urlcolor: blue
---

\pagestyle{fancy}
\fancyhf{}
\rhead{Will Nickols}
\lhead{Section 6}
\rfoot{Page \thepage}

# Announcements

\begin{wrapfigure}{r}{0.12\textwidth}
  \centering
    \vspace*{-1.3cm}
    \includegraphics[width=\linewidth]{section_qr_code.png}
\end{wrapfigure}

Make sure to sign in on the [google form](https://forms.gle/xm1DfzuZFNcWU6fH8) (I send a list of which section questions are useful for which pset questions afterwards)

Pset 5 due Friday 10/27

# Introductions
- One question or thought related to lecture last week (interactions, polynomials, smoothers)

```{r, echo=F, warning=F, message=F, cache=F}
list.of.packages <- c("ggplot2", "gridExtra", "MASS", "reshape2", "dplyr")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

library(ggplot2)
library(gridExtra)
library(MASS)
library(reshape2)
library(dplyr)
```

# Correlated transformations

Transformations and polynomials are useful, but they often create additional correlation in the model. Suppose we have two increasing functions $g$ and $h$, and let $X_1$ and $X_2$ be i.i.d. continuous random variables.

1. Explain why $$(g(X_1)-g(X_2))(h(X_1)-h(X_2))>0$$

\vspace{3 cm}

2. Take the expectation of both sides and expand to show that $\textrm{Cov}(g(X),h(X))>0$ for a random variable $X$ with the same distribution as $X_1$ and $X_2$.

\vspace{6 cm}

3. Suppose you have some continuous predictor $X$ and two increasing transformations of $X$ you include in the model. What does this say about the transformed predictors?

\vspace{3 cm}

4. Suppose you have a strictly positive continuous predictor and you include it as a polynomial. What can you say about the $X, X^2, X^3, ...$ coefficients?

\vspace{3 cm}

\newpage

# Groups and polynomials on real data

These problems will deal with a dataset of country-level statistics from [UNdata](https://data.un.org/), [Varieties of Democracy](https://v-dem.net/data/the-v-dem-dataset/), and the [World Bank](https://data.worldbank.org/indicator/AG.LND.PRCP.MM).

```{r, cache=T, warning=FALSE, echo=F}
countries <- read.csv("data/country_stats.csv", check.names = F)
countries_2010 <- countries[countries$Year == 2010,]
```

1. With `Northern America` as the reference group, a regression model is fit to predict a country's GDP per capita from its region. Interpret the coefficients.

```{r, cache=T, warning=FALSE, echo=F}
countries_2010$Region <- as.factor(countries_2010$Region)
countries_2010$Region <- relevel(countries_2010$Region, ref = "Northern America")
summary(lm(countries_2010$`GDP per capita (US dollars)`~Region, countries_2010))$coefficients
```
\vspace{3 cm}

2. The following 2nd order polynomial regression model predicts the percent of arable land in a country from its average annual precipitation. What is the optimal precipitation for having the most arable land?

```{r, echo=F, fig.width=4.5, fig.height=3, fig.align='center', cache=T, warning=F}
library(ggplot2)

lm1 <- lm(`Arable land (% of total land area)`~poly(`Precipitation (mm)`, 2, raw = TRUE), countries_2010)
coefficients(summary(lm1))

ggplot(countries, aes(x=`Precipitation (mm)`, y=`Arable land (% of total land area)`)) + 
  geom_point() + 
  stat_smooth(method = "lm",
              formula = y ~ poly(x, 2)) + 
  ylim(0, 60) + 
  theme_bw()
```
\vspace{4 cm}

3. Use the previous model to find the probability that a country with $x$ mm annual precipitation will have less than $\tau$ percent of its land arable. Recall that $$T=\frac{Y-\vv{X}_0^T\vv{\hat{\beta}}}{\hat{\sigma}\sqrt{1+\vv{X}_0^T(\mathbf{X}^T\mathbf{X})^{-1}\vv{X}_0}}\sim t_{n-(p+1)}$$ where $\vv{X}_0$ is the new vector of predictors, $\mathbf{X}$ is the matrix of previous predictors, and $Y$ is the new outcome assuming it follows it the previous model.

\vspace{9 cm}

4. Compare the prediction accuracy of a LOESS model to that of the previous model.

```{r, echo=F, fig.width=4.5, fig.height=3, fig.align='center', cache=T, warning=F}
loess_model <- loess(countries_2010$`Arable land (% of total land area)`~countries_2010$`Precipitation (mm)`)

sse <- function(x) {return(sum((x - mean(x))^2))}

# LM Rsq
lm_rsq <- 1-sum(residuals(lm1)^2)/
  sse(countries_2010[!is.na(countries_2010$`Arable land (% of total land area)`) & 
                       !is.na(countries_2010$`Precipitation (mm)`),]$`Arable land (% of total land area)`)

# Loess Rsq
loess_rsq <- 1-sum(residuals(loess_model)^2)/
  sse(countries_2010[!is.na(countries_2010$`Arable land (% of total land area)`) & 
                       !is.na(countries_2010$`Precipitation (mm)`),]$`Arable land (% of total land area)`)

ggplot(countries, aes(x=`Precipitation (mm)`, y=`Arable land (% of total land area)`)) + 
  geom_point() + 
  stat_smooth(method = "lm",
              formula = y ~ poly(x, 2)) + 
  stat_smooth(method="loess", 
              formula = y ~ x,
              col="red") + 
  ylim(0, 60) + 
  theme_bw()

print(round(c("LM R2" = lm_rsq, "LOESS R2" = loess_rsq), 3))
```

\vspace{4 cm}

5. Perform a formal hypothesis test to determine whether a fourth degree polynomial fits the data better than a second degree polynomial.

```{r, echo=F, fig.width=4.5, fig.height=3, fig.align='center', cache=T, warning=F}
ggplot(countries, aes(x=`Precipitation (mm)`, y=`Arable land (% of total land area)`)) + 
  geom_point() + 
  stat_smooth(method = "lm",
              formula = y ~ poly(x, 2)) + 
  stat_smooth(method = "lm",
              formula = y ~ poly(x, 4), col="red") + 
  ylim(0, 60) + 
  theme_bw()

lm2 <- lm(`Arable land (% of total land area)`~poly(`Precipitation (mm)`, 4, raw = TRUE), countries_2010)
anova(lm1, lm2)
```

\vspace{4 cm}

6. Interpret the following model that looks at the proportion of the national parliament that is women as a function of GDP per capita and whether the country is a democracy.

```{r, echo=F, fig.width=4.5, fig.height=3, fig.align='center', cache=T, warning=F}
countries_2010$is_democracy <- as.factor(countries_2010$v2x_polyarchy > 0.5)

countries_2010_with_dem <- countries_2010[!is.na(countries_2010$is_democracy),]

lm3 <- lm(`Seats held by women in national parliament, as of February (%)` ~ 
            log2(`GDP per capita (US dollars)`) * is_democracy, countries_2010_with_dem)
summary(lm3)$coefficients

ggplot(countries_2010_with_dem, aes(x=log2(`GDP per capita (US dollars)`), y = `Seats held by women in national parliament, as of February (%)`, 
                           col = is_democracy)) + 
  geom_point() + 
  stat_smooth(method="lm",
              formula = y~x) + 
  theme_bw() + 
  labs(col="Democracy?") + 
  ylab("National parliament % women")
```

\vspace{3 cm}

\newpage

# Simpson's simulation

1. For the following data table, write out the design matrix that would be used in the following model: `response ~ category * value`.

```{r, cache=T, warning=FALSE, echo=F}
set.seed(139)
library(knitr)
library(dplyr)

n = 10
value = round(rnorm(n, 5, 2), 1)
category = as.factor(sample(c(1,2,3), n, replace = T))
df <- data.frame(Response = round(case_when(category == "1" ~ 3,
                                      category == "2" ~ 7,
                                      category == "3" ~ 2) * value + rnorm(n, 0, 5), 1),
                 Category=category,
                 Value=value)

kable(df)
```

\vspace{8 cm}

2. For each of the pairs of plots below, determine what model should be fit to best describe the data (e.g., `response ~ x^2 + category`).

```{r, echo=F, cache=T, warning=F, fig.height=3.5}
library(gridExtra)

# Sample size
n <- 1000

# Five categories
category <- sample(1:5, n, replace = T)

# Different x mean for each category
x <- rnorm(n, 10 * category, 15)

# Generate the response variable
response = 100 - (40 * category) + x * 1 + rnorm(n, 0, 15)

# Make a dataframe for plotting
df <- data.frame(x = x,
                 category = as.factor(category),
                 response = response)

# Models with and without category as a predictor
plot1 <- ggplot(df, aes(x = x, y = response)) + 
  geom_point() + 
  theme_bw() + 
  geom_smooth(method="lm", formula = y~x) + 
  ylab("Y") + 
  xlab("X")

plot2 <- ggplot(df, aes(x = x, y = response, color = category)) + 
  geom_point() + 
  theme_bw() + 
  geom_smooth(method="lm", formula = y~x) + 
  ylab("Y") + 
  xlab("X") + 
  labs(color = "Category")

# Plot models
grid.arrange(plot1, plot2, ncol = 2, widths=c(0.8,1))

# Correctly fit model
# summary(lm(response ~ x + category, df))$coefficients
```

\vspace{2 cm}

```{r, echo=F, cache=T, warning=F, fig.height=3.5}
n <- 1000

# Two categories
category <- sample(0:1, n, replace = T)

# Generate predictor
x <- rnorm(n, 0, 15)

# Generate response variable
response = x * (-1 + 2 * category) + rnorm(n, 0, 15)

# Data frame for plotting
df <- data.frame(x = x,
                 category = as.factor(category),
                 response = response)

# Models with and without interaction terms
plot1 <- ggplot(df, aes(x = x, y = response)) + 
  geom_point() + 
  theme_bw() + 
  geom_smooth(method="lm", formula = y~x) + 
  ylab("Y") + 
  xlab("X")

plot2 <- ggplot(df, aes(x = x, y = response, color = category)) + 
  geom_point() + 
  theme_bw() + 
  geom_smooth(method="lm", formula = y~x) + 
  ylab("Y") + 
  xlab("X") + 
  labs(color = "Category")

grid.arrange(plot1, plot2, ncol = 2, widths=c(0.8,1))

# Correctly fit model
# summary(lm(response ~ x * category, df))$coefficients
```

\vspace{2 cm}

3. Name a reason to avoid fitting many interaction terms right from the beginning.

\vspace{3 cm}

\newpage

# ANOVA as a linear model

Let $Y_{ij}$ be data point $j$ from group $i$ where there are $k$ groups with $n_i$ data points in group $i$. Imagine we run an ANOVA as well as an $F$-test for overall significance of a regression model with only the categories as predictors. Recall the original ANOVA $F$-statistic: $$\frac{\sum_{i=1}^kn_i(\bar{Y}_i-\bar{Y})^2/(k-1)}{\sum_{i=1}^k\sum_{j=1}^{n_i}(Y_{ij}-\bar{Y_i})^2/(n-k)}$$ and the overall regression $F$-statistic: $$\frac{\sum_{i,j}(\hat{Y}_{ij}-\bar{Y})^2/p}{\sum_{i,j}(Y_{ij}-\hat{Y}_{ij})^2/(n-p-1)}$$ where $p$ is the number of predictors (not including the intercept in the model).

1. What is $p$ in terms of $k$?

\vspace{2 cm}

2. What is $\hat{Y}_{ij}$?  Why is this the case?

\vspace{3 cm}

3. Show that the two $F$-statistics are equal.

\vspace{7 cm}