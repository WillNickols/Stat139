---
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage{tcolorbox}
- \usepackage{fancyhdr}
- \usepackage[utf8]{inputenc}
- \usepackage{wrapfig}
- \usepackage{amsmath}
- \usepackage{booktabs}
- \usepackage{esvect}
urlcolor: blue
---

\pagestyle{fancy}
\fancyhf{}
\rhead{Will Nickols}
\lhead{Section 6}
\rfoot{Page \thepage}

# Announcements

\begin{wrapfigure}{r}{0.12\textwidth}
  \centering
    \vspace*{-1.3cm}
    \includegraphics[width=\linewidth]{section_qr_code.png}
\end{wrapfigure}

Make sure to sign in on the [google form](https://forms.gle/xm1DfzuZFNcWU6fH8) (I send a list of which section questions are useful for which pset questions afterwards)

Pset 6 due Friday 10/27

# Introductions
- One question or thought related to lecture last week (Binary predictors, interactions, polynomials, smoothers)

```{r, echo=F, warning=F, message=F, cache=F}
list.of.packages <- c("ggplot2", "gridExtra", "MASS", "reshape2", "dplyr")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

library(ggplot2)
library(gridExtra)
library(MASS)
library(reshape2)
library(dplyr)
```

# Correlated transformations

Transformations and polynomials are useful, but they often create additional correlation in the model. Suppose we have two increasing functions $g$ and $h$, and let $X_1$ and $X_2$ be i.i.d. continuous random variables.

1. Explain why $$(g(X_1)-g(X_2))(h(X_1)-h(X_2))>0$$

If $X_1>X_2$, $g(X_1)>g(X_2)$ and $h(X_1)>h(X_2)$, so a positive times a positive is positive. If $X_1<X_2$, $g(X_1)<g(X_2)$ and $h(X_1)<h(X_2)$, and a negative times a negative is positive.

2. Take the expectation of both sides and expand to show that $\textrm{Cov}(g(X),h(X))>0$ for a random variable $X$ with the same distribution as $X_1$ and $X_2$.

$$\begin{aligned}
E((g(X_1)-g(X_2))(h(X_1)-h(X_2)))&=E(g(X_1)h(X_1)-g(X_1)h(X_2)-g(X_2)h(X_1)+g(X_2)h(X_2)))\\
&=E(g(X_1)h(X_1)+g(X_2)h(X_2))-E(g(X_1))E(h(X_2))-E(g(X_2))E(h(X_1))\\
&=2E(g(X)h(X))-2E(g(X))E(h(X))\\
&=2\textrm{Cov}(g(X),h(X))
\end{aligned}$$

which implies $\textrm{Cov}(g(X),h(X))>0$.

3. Suppose you have some continuous predictor $X$ and two increasing transformations of $X$ you include in the model. What does this say about the transformed predictors?

They will be positively correlated, creating multicollinearity that could strongly affect the slopes relative to a model with only one of the transformations.

4. Suppose you have a strictly positive continuous predictor and you include it as a polynomial. What can you say about the $X, X^2, X^3, ...$ coefficients?

They will all be positively correlated since $g(x)=x^n$ is a strictly increasing function for $x>0$.

\newpage

# Groups and polynomials on real data

These problems will deal with a dataset of country-level statistics from [UNdata](https://data.un.org/), [Varieties of Democracy](https://v-dem.net/data/the-v-dem-dataset/), and the [World Bank](https://data.worldbank.org/indicator/AG.LND.PRCP.MM).

```{r, cache=T, warning=FALSE, echo=F}
countries <- read.csv("data/country_stats.csv", check.names = F)
countries_2010 <- countries[countries$Year == 2010,]
```

1. With `Northern America` as the reference group, a regression model is fit to predict a country's GDP per capita from its region. Interpret the coefficients.

```{r, cache=T, warning=FALSE, echo=F}
countries_2010$Region <- as.factor(countries_2010$Region)
countries_2010$Region <- relevel(countries_2010$Region, ref = "Northern America")
summary(lm(countries_2010$`GDP per capita (US dollars)`~Region, countries_2010))$coefficients
```
North American countries have an average GDP per capita of \$60429.75. This is significantly more (pairwise) than every other region except Oceania and West Europe. The average GDP per capita in the other regions can be calculated by adding their coefficients to the intercept. For example, the Caribbean has an average GDP per capita of $60429.75+(-42178.39)=18251.36$.

2. The following 2nd order polynomial regression model predicts the percent of arable land in a country from its average annual precipitation. What is the optimal precipitation for having the most arable land?

```{r, echo=F, fig.width=4.5, fig.height=3, fig.align='center', cache=T, warning=F}
library(ggplot2)

lm1 <- lm(`Arable land (% of total land area)`~poly(`Precipitation (mm)`, 2, raw = TRUE), countries_2010)
coefficients(summary(lm1))

ggplot(countries, aes(x=`Precipitation (mm)`, y=`Arable land (% of total land area)`)) + 
  geom_point() + 
  stat_smooth(method = "lm",
              formula = y ~ poly(x, 2)) + 
  ylim(0, 60) + 
  theme_bw()
```
Setting the derivative of the fit line to 0 gives $$2\hat{\beta_2}x+\hat{\beta}_1=0\implies x=\frac{\hat{\beta_1}}{-2\hat{\beta_2}}=1256.4$$ 
Therefore, the estimated proportion of arable land increases with increasing precipitation up to 1256.4 mm per year, after which it declines. This makes sense because areas with either very little rainfall or a lot of rainfall are unlikely to have much productive farmland. Interestingly, the annual average precipitation in Cambridge is [1174 mm](https://en.climate-data.org/north-america/united-states-of-america/massachusetts/cambridge-1734/).

3. Use the previous model to find the probability that a country with $X'$ mm annual precipitation will have less than $\tau$ percent of its land arable. Recall that $$T=\frac{Y'-\vv{X}_0^T\vv{\hat{\beta}}}{\hat{\sigma}\sqrt{1+\vv{X}_0^T(\mathbf{X}^T\mathbf{X})^{-1}\vv{X}_0}}\sim t_{n-(p+1)}$$ where $\vv{X}_0$ is the new vector of predictors and $\mathbf{X}$ is the matrix of previous predictors.

Let $\vv{X}_0^T=[1, X', X'^2]$.

$$\begin{aligned}P(Y'<\tau)&=P\left(\frac{Y'-\vv{X}_0^T\vv{\hat{\beta}}}{\hat{\sigma}\sqrt{1+\vv{X}_0^T(\mathbf{X}^T\mathbf{X})^{-1}\vv{X}_0}}<\frac{\tau-\vv{X}_0^T\vv{\hat{\beta}}}{\hat{\sigma}\sqrt{1+\vv{X}_0^T(\mathbf{X}^T\mathbf{X})^{-1}\vv{X}_0}}\right)\\
&=P\left(T<\frac{\tau-\vv{X}_0^T\vv{\hat{\beta}}}{\hat{\sigma}\sqrt{1+\vv{X}_0^T(\mathbf{X}^T\mathbf{X})^{-1}\vv{X}_0}}\right)\\
&=F_{t_{n-(p+1)}}\left(\frac{\tau-\vv{X}_0^T\vv{\hat{\beta}}}{\hat{\sigma}\sqrt{1+\vv{X}_0^T(\mathbf{X}^T\mathbf{X})^{-1}\vv{X}_0}}\right)
\end{aligned}$$

For example, we can look at the probability a country with 1500 mm precipitation per year on average will have less than 10\% of its land arable.

```{r, cache=T, echo=F}
threshold = 10

X_0 = c(1, 1500, 1500^2)

# Using the null distribution, we can standardize the threshold and find the density below it
pt((threshold - X_0 %*% coef(lm1)) / 
     (sqrt(t(X_0) %*% vcov(lm1) %*% X_0 + summary(lm1)$sigma^2)), 
   lm1$df.residual)
```


```{r, include=F, cache=TRUE}
# Check that this works as intended
prediction <- predict(lm1,newdata = data.frame('Precipitation (mm)'=1500, check.names = F),interval = "prediction")
threshold = prediction[2:3]

# These should threshold above 0.025 and 0.975 density 
pt((threshold - as.vector(X_0 %*% coef(lm1))) / 
     as.vector(sqrt(t(X_0) %*% vcov(lm1) %*% X_0 + summary(lm1)$sigma^2)), 
   lm1$df.residual)
```

4. Compare the prediction accuracy of a LOESS model to that of the previous model.

```{r, echo=F, fig.width=4.5, fig.height=3, fig.align='center', cache=T, warning=F}
loess_model <- loess(countries_2010$`Arable land (% of total land area)`~countries_2010$`Precipitation (mm)`)

sse <- function(x) {return(sum((x - mean(x))^2))}

# LM Rsq
lm_rsq <- 1-sum(residuals(lm1)^2)/
  sse(countries_2010[!is.na(countries_2010$`Arable land (% of total land area)`) & 
                       !is.na(countries_2010$`Precipitation (mm)`),]$`Arable land (% of total land area)`)

# Loess Rsq
loess_rsq <- 1-sum(residuals(loess_model)^2)/
  sse(countries_2010[!is.na(countries_2010$`Arable land (% of total land area)`) & 
                       !is.na(countries_2010$`Precipitation (mm)`),]$`Arable land (% of total land area)`)

ggplot(countries, aes(x=`Precipitation (mm)`, y=`Arable land (% of total land area)`)) + 
  geom_point() + 
  stat_smooth(method = "lm",
              formula = y ~ poly(x, 2)) + 
  stat_smooth(method="loess", 
              formula = y ~ x,
              col="red") + 
  ylim(0, 60) + 
  theme_bw()

print(round(c("LM R2" = lm_rsq, "LOESS R2" = loess_rsq), 3))
```
The LOESS model's $R^2$ value is more than double the linear model's $R^2$, so it is explaining much more of the variance.

5. Perform a formal hypothesis test to determine whether a fourth degree polynomial fits the data better than a second degree polynomial.

```{r, echo=F, fig.width=4.5, fig.height=3, fig.align='center', cache=T, warning=F}
ggplot(countries, aes(x=`Precipitation (mm)`, y=`Arable land (% of total land area)`)) + 
  geom_point() + 
  stat_smooth(method = "lm",
              formula = y ~ poly(x, 2)) + 
  stat_smooth(method = "lm",
              formula = y ~ poly(x, 4), col="red") + 
  ylim(0, 60) + 
  theme_bw()

lm2 <- lm(`Arable land (% of total land area)`~poly(`Precipitation (mm)`, 4, raw = TRUE), countries_2010)
anova(lm1, lm2)
```
In an ESS $F$-test of $H_0: \beta_{\textrm{Precipitation}^3}=\beta_{\textrm{Precipitation}^4}=0$ (the fourth degree polynomial is no better) vs $H_a:$ at least one of the coefficients is not 0, we get an $F$-statistic of 13.3 for an $F_{2, 174}$ distribution, yielding a p-value of $4.3\times 10^{-6}$. Therefore, we reject the null and conclude that the fourth degree polynomial is significantly better than the second degree polynomial. (Note that the fourth-degree polynomial looks like the LOESS model that performed much better.)

6. Interpret the following model that looks at the proportion of the national parliament that is women as a function of GDP per capita and whether the country is a democracy.

```{r, echo=F, fig.width=4.5, fig.height=3, fig.align='center', cache=T, warning=F}
countries_2010$is_democracy <- as.factor(countries_2010$v2x_polyarchy > 0.5)

countries_2010_with_dem <- countries_2010[!is.na(countries_2010$is_democracy),]

lm3 <- lm(`Seats held by women in national parliament, as of February (%)` ~ 
            log2(`GDP per capita (US dollars)`) * is_democracy, countries_2010_with_dem)
summary(lm3)$coefficients

ggplot(countries_2010_with_dem, aes(x=log2(`GDP per capita (US dollars)`), y = `Seats held by women in national parliament, as of February (%)`, 
                           col = is_democracy)) + 
  geom_point() + 
  stat_smooth(method="lm",
              formula = y~x) + 
  theme_bw() + 
  labs(col="Democracy?") + 
  ylab("National parliament % women")
```

In non-democracies, a doubling in GDP per capita is significantly associated with a 1.3\% decrease in the proportion of the parliament that is women. In democracies, a doubling in GDP per capita is significantly associated with a -1.3+3.2=1.9\% increase in the proportion of parliament that is women.

\newpage

# ANOVA as a linear model

Let $Y_{ij}$ be data point $j$ from group $i$ where there are $k$ groups with $n_i$ data points in group $i$. Imagine we run an ANOVA as well as an $F$-test for overall significance of a regression model with only the categories as predictors. Recall the original ANOVA $F$-statistic: $$\frac{\sum_{i=1}^kn_i(\bar{Y}_i-\bar{Y})^2/(k-1)}{\sum_{i=1}^k\sum_{j=1}^{n_i}(Y_{ij}-\bar{Y_i})^2/(n-k)}$$ and the overall regression $F$-statistic: $$\frac{\sum_{i,j}(\hat{Y}_{ij}-\bar{Y})^2/p}{\sum_{i,j}(Y_{ij}-\hat{Y}_{ij})^2/(n-p-1)}$$ where $p$ is the number of predictors (not including the intercept in the model).

1. What is $p$ in this case?

We have $k$ groups, but one will be set as the baseline (intercept), so we have $p=k-1$ predictors not including the intercept.

2. What is $\hat{Y}_{ij}$?  Why is this the case?

$\hat{Y}_{ij}=\bar{Y_i}$ because the only non-zero values in $\vv{X_{ij}}$ for all $(\vv{X_{ij}},Y_{ij})$ in group $i$ are a 1 in the $i^{th}$ position if $i\neq 1$ and a single 1 in the intercept position. Therefore, the predicted value will have to be the same for all $Y_{ij}$ in group $i$, and the prediction that minimizes the squared differences with respect to these observations is the mean of the group $\bar{Y_i}$.

3. Show that the two $F$-statistics are equal.

Using the $p$ we found earlier and the fact that $\hat{Y}_{ij}=\bar{Y_i}$,

$$\begin{aligned}
\frac{\sum_{i,j}(\hat{Y}_{ij}-\bar{Y})^2/p}{\sum_{i,j}(Y_{ij}-\hat{Y}_{ij})^2/(n-p-1)}&=\frac{\sum_{i,j}(\hat{Y}_{ij}-\bar{Y})^2/(k-1)}{\sum_{i,j}(Y_{ij}-\hat{Y}_{ij})^2/(n-k)}\\
&=\frac{\sum_{i=1}^kn_i(\bar{Y}_i-\bar{Y})^2/(k-1)}{\sum_{i=1}^k\sum_{j=1}^{n_i}(Y_{ij}-\bar{Y_i})^2/(n-k)}
\end{aligned}$$

\newpage

# Simpson's simulation

1. For the following data table, write out the design matrix that would be used in the following model: `response ~ category * value`.

```{r, cache=T, warning=FALSE, echo=F}
set.seed(139)
library(knitr)
library(dplyr)

n = 10
value = round(rnorm(n, 5, 2), 1)
category = as.factor(sample(c(1,2,3), n, replace = T))
df <- data.frame(Response = round(case_when(category == "1" ~ 3,
                                      category == "2" ~ 7,
                                      category == "3" ~ 2) * value + rnorm(n, 0, 5), 1),
                 Category=category,
                 Value=value)

kable(df)
```

```{r, cache=T, echo=F}
lm_sim <- lm(Response ~ Category * Value, df)
model_matrix_out = model.matrix(lm_sim)
attr(model_matrix_out, "assign") <- NULL
attr(model_matrix_out, "contrasts") <- NULL

print(model_matrix_out)
```

2. For each of the pairs of plots below, determine what model should be fit to best describe the data (e.g., `response ~ x^2 + category`).

```{r, echo=F, cache=T, warning=F, fig.height=3.5}
library(gridExtra)

# Sample size
n <- 1000

# Five categories
category <- sample(1:5, n, replace = T)

# Different x mean for each category
x <- rnorm(n, 10 * category, 15)

# Generate the response variable
response = 100 - (40 * category) + x * 1 + rnorm(n, 0, 15)

# Make a dataframe for plotting
df <- data.frame(x = x,
                 category = as.factor(category),
                 response = response)

# Models with and without category as a predictor
plot1 <- ggplot(df, aes(x = x, y = response)) + 
  geom_point() + 
  theme_bw() + 
  geom_smooth(method="lm", formula = y~x)

plot2 <- ggplot(df, aes(x = x, y = response, color = category)) + 
  geom_point() + 
  theme_bw() + 
  geom_smooth(method="lm", formula = y~x)

# Plot models
grid.arrange(plot1, plot2, ncol = 2, widths=c(0.8,1))

# Correctly fit model
# summary(lm(response ~ x + category, df))$coefficients
```

Because each group appears to have a similar slope but distinct intercept, we should use `response ~ x + category`.

```{r, echo=F, cache=T, warning=F, fig.height=3.5}
n <- 1000

# Two categories
category <- sample(0:1, n, replace = T)

# Generate predictor
x <- rnorm(n, 0, 15)

# Generate response variable
response = x * (-1 + 2 * category) + rnorm(n, 0, 15)

# Data frame for plotting
df <- data.frame(x = x,
                 category = as.factor(category),
                 response = response)

# Models with and without interaction terms
plot1 <- ggplot(df, aes(x = x, y = response)) + 
  geom_point() + 
  theme_bw() + 
  geom_smooth(method="lm", formula = y~x)

plot2 <- ggplot(df, aes(x = x, y = response, color = category)) + 
  geom_point() + 
  theme_bw() + 
  geom_smooth(method="lm", formula = y~x)

grid.arrange(plot1, plot2, ncol = 2, widths=c(0.8,1))

# Correctly fit model
# summary(lm(response ~ x * category, df))$coefficients
```

Because each group appears to have a different slope, we should use `response ~ x * category`.

3. Name a reason to avoid fitting many interaction terms right from the beginning.

Fitting more terms (especially with little predictive power) increases overfitting and increases your estimated standard error (since $\hat{\sigma}^2=\textrm{SSE}/(n-p-1)$). Therefore, the slopes that actually matter will lose significance, and the model will be worse at fitting new data.


