---
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage{tcolorbox}
- \usepackage{fancyhdr}
- \usepackage[utf8]{inputenc}
- \usepackage{wrapfig}
- \usepackage{amsmath}
- \usepackage{booktabs}
- \usepackage{esvect}
urlcolor: blue
---

\pagestyle{fancy}
\fancyhf{}
\rhead{Will Nickols}
\lhead{Section 7}
\rfoot{Page \thepage}

# Announcements

\begin{wrapfigure}{r}{0.12\textwidth}
  \centering
    \vspace*{-1.3cm}
    \includegraphics[width=\linewidth]{section_qr_code.png}
\end{wrapfigure}

Make sure to sign in on the [google form](https://forms.gle/xm1DfzuZFNcWU6fH8) (I send a list of which section questions are useful for which pset questions afterwards)

Pset 6 due Friday 11/3

# Introductions
- One question or thought related to lecture last week (Model comparison, sequential variable selection, AIC, BIC)

```{r, echo=F, warning=F, message=F, cache=F}
list.of.packages <- c("caret", "gam", "ggplot2", "MASS", "dplyr", "knitr")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

library(ggplot2)
library(caret)
library(gam)
library(MASS)
library(dplyr)
library(knitr)
```

# Step procedures and cross validation

1. Given the following table, find the model produced by forward selection using an ESS $F$-test and starting from a model with only an intercept. (You should be able to do this with only a single test.)

\begin{center}
\begin{tabular}{ccc}
\bf{Model} & \bf{Residual sum} & \bf{Degrees} \\
\bf{Variables} & \bf{of squares} & \bf{of freedom} \\
\hline
None & 7,200 & 38 \\
$X_1$ & 6,600 & 37 \\
$X_2$ & 6,980 & 37 \\
$X_3$ & 6,760 & 37 \\
\end{tabular}
\end{center}

\vspace{3 cm}

These problems will deal with a dataset of country-level statistics from [UNdata](https://data.un.org/), [Varieties of Democracy](https://v-dem.net/data/the-v-dem-dataset/), and the [World Bank](https://data.worldbank.org/indicator/AG.LND.PRCP.MM).

```{r, cache=T, warning=FALSE, echo=F}
countries <- read.csv("data/country_stats.csv", check.names = F)
countries_2010 <- countries[countries$Year == 2010,]
```

2. The next three questions will run forwards, backwards, and both-direction variable selection procedures. Predict which model will have the highest $R^2$.

\vspace{2 cm}

3. The following runs a forward variable selection procedure to predict log2 GDP per capita in 2010 from a country's urban population, its proportion of people 60+, its patents in force, its arable land, its energy supply, and its unemployment rate. The procedure starts with an intercept only model and uses an upper scope of all the two-way interaction terms for the variables listed above. The final model is shown along with the $R^2$ and AIC. How many coefficients are retained? Are the p-values reliable?

```{r, cache=TRUE, echo=F}
countries_subset <- countries_2010[!is.na(log2(countries_2010$`GDP per capita (US dollars)`)) & 
                                   !is.na(countries_2010$`Urban population (percent)`) &
                                   !is.na(countries_2010$`Population aged 60+ years old (percentage)`) &
                                     !is.na(countries_2010$`Patents in force (number)`) &
                                     !is.na(countries_2010$`Arable land (% of total land area)`) &
                                     !is.na(countries_2010$`Supply per capita (gigajoules)`) &
                                     !is.na(countries_2010$`Unemployment rate - Total`),]

colnames(countries_subset) <- case_when(colnames(countries_subset) == "GDP per capita (US dollars)" ~ "GDP",
                                        colnames(countries_subset) == "Urban population (percent)" ~ "Urban",
                                        colnames(countries_subset) == "Population aged 60+ years old (percentage)" ~ "60+",
                                        colnames(countries_subset) == "Patents in force (number)" ~ "Patents",
                                        colnames(countries_subset) == "Arable land (% of total land area)" ~ "Arable",
                                        colnames(countries_subset) == "Supply per capita (gigajoules)" ~ "Energy",
                                        colnames(countries_subset) == "Unemployment rate - Total" ~ "Unemployment",
                                        TRUE ~ colnames(countries_subset)
                                        )

lm1 <- lm(log2(GDP) ~ 1, countries_subset)
step_model_1 <- step(lm1, 
                     scope = list(upper = formula(lm(log2(GDP) ~ (Urban + `60+` + Patents + Arable + Energy + Unemployment)^2, countries_subset))), 
                     direction = "forward", trace = F)
summary(step_model_1)$coefficients
c("R2" = round(summary(step_model_1)$r.squared, 3), "AIC" = round(AIC(step_model_1), 3))
```

\vspace{3 cm}

4. The following runs a backwards variable selection procedure. The procedure starts with all the two-way interaction terms for the variables listed above. The final model is shown along with the $R^2$ and AIC. Note how the number of coefficients changes.

```{r, cache=TRUE, echo=F}
step_model_2 <- step(lm(log2(GDP) ~ (Urban + `60+` + Patents + Arable + Energy + Unemployment)^2, countries_subset), 
                     scope = list(lower=formula(log2(GDP) ~ 1)), 
                     direction = "backward", trace = F)
summary(step_model_2)$coefficients
c("R2" = round(summary(step_model_2)$r.squared, 3), "AIC" = round(AIC(step_model_2), 3))
```
\vspace{3 cm}

5. The following runs a both-direction variable selection procedure. It starts with all the coefficients and has an upper bound of all interactions. The final model is shown along with the $R^2$ and AIC. How does this model compare to the ones above?

```{r, cache=TRUE, echo=F}
step_model_3 <- step(lm(log2(GDP) ~ Urban + `60+` + Patents + Arable + Energy + Unemployment, countries_subset), 
                     scope = list(upper=formula(log2(GDP) ~ (Urban + `60+` + Patents + Arable + Energy + Unemployment)^2),
                                  lower=formula(log2(GDP) ~ 1)), 
                     direction = "both", trace = F)

summary(step_model_3)$coefficients
c("R2" = round(summary(step_model_3)$r.squared, 3), "AIC" = round(AIC(step_model_3), 3))
```
\vspace{3 cm}

6. Based on AIC, which model is the best?  Why didn't the other procedures find the same model?

\vspace{3 cm}

7. Recall from last week that we looked at various models predicting the proportion of arable land from the precipitation.

Here, we run $k$-fold cross validation to estimate out-of-sample RMSE for a LOESS model and a degree 2 polynomial model to predict the proportion of arable land from the country's average annual precipitation. Which model performs better for each $k$?

```{r, warning=F, cache=T, echo=F}
library(caret)
library(gam)
set.seed(139)

colnames(countries_2010) <- case_when(colnames(countries_2010) == "Arable land (% of total land area)" ~ "Arable",
                                        colnames(countries_2010) == "Precipitation (mm)" ~ "Precipitation",
                                        TRUE ~ colnames(countries_2010)
                                        )

poly_model <- vector(length = 3)
i <- 1
ks <- c(10, 20, 50, 100, 200)
for (ncross in ks) {
  poly_model[i] <- train(Arable~poly(Precipitation, 2, raw = TRUE), data = countries_2010,
      method = "lm",
      trControl = trainControl(method = "cv", number = ncross), 
      na.action = na.omit)$results[2][[1]]
  i <- i + 1
}

loess_model <- vector(length = 3)
i <- 1
for (ncross in ks) {
  loess_model[i] <- train(Arable~Precipitation, data = countries_2010,
      method = "gamLoess",
      trControl = trainControl(method = "cv", number = ncross), 
      na.action = na.omit)$results[3][[1]]
  i <- i + 1
}

kable(data.frame("k"=ks, "polynomial"=round(poly_model, 3), "loess"=round(loess_model, 3)))
```
\vspace{3 cm}

\newpage

# From BIC to SSE

In a linear model, the Bayes Information Criterion has a nice interpretation as a trade-off between the sum of squares error $SSE=\sum_{i=1}^n(Y_i-\hat{Y_i})^2$ and the number of predictors $p$. In particular, $$\textrm{BIC} = n\log(\textrm{SSE}/n)+(p+2)\log(n)$$
In this problem, we'll derive the result for ourselves.

1. First, recall that for a multiple regression model, $Y_i=\beta_0+\beta_1X_{1,i}+...+\beta_pX_{p, i}+\epsilon_i$ with $\epsilon_i\sim\mathcal{N}(0, \sigma^2)$. Also recall that for this distributional assumption, $\hat{\vv{\beta}}$ is the set of parameters that maximize the likelihood function of the whole model. Lastly, recall that in a multiple regression model, the *maximum likelihood estimate* for the residual variance is $\hat\sigma^2=\frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{n}$ (and note that this is different from our unbiased estimator). Write the maximized likelihood function for the observed data as a function of $\hat{y_i}$, $y_i$, and $\hat\sigma^2$.

\vspace{3 cm}

2. Write the maximized log likelihood function of the observed data as a function of the extra sum of squares ($\textrm{SSE}$). (You will find there are two terms that are constant regardless of the predictors; these can be dropped because we are only interested in comparing AIC between models.)

\vspace{8 cm}

3. Find the Bayes Information Criterion of the fit model. Recall that with $\hat{L}$ as the maximized likelihood function, $$\textrm{BIC} = (p+2)\log(n)-2\log(\hat{L})$$

\vspace{3 cm}

```{r, cache=T, warning=FALSE, include=F}
n <- 100
p <- 10
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
betas <- rnorm(p) * 5
y <- X %*% betas + rnorm(n)
model1 <- lm(y ~ X)

# R's AIC
extractAIC(model1, k=log(length(model1$residuals)))[2]

# AIC by hand
n <- length(model1$residuals)
(p+2) * log(n) + n * log(sum(model1$residuals^2)/n)
```

\newpage

# AIC as hypothesis testing

Suppose we're performing stepwise variable selection on a linear model with coefficients $\beta_0,...,\beta_k$ and we want to compare it to a model with $\beta_0,...,\beta_{k+1}$. Here, we'll use $$\textrm{AIC}=2(p+2)-2\log(\hat{L})$$

1. Recall from Stat 111 the likelihood ratio test: under the null $H_0:\theta=\theta_0$ with $\hat{\theta}$ as the MLE, asymptotically 
$$\Lambda(\vv{y})=2\left(\log L(\hat{\theta},\vv{y})-\log L(\theta_0,\vv{y})\right)\sim\chi^2_{1}$$
Write the equivalent null hypothesis and null test statistic distribution for testing whether $\beta_{k+1}=0$.

\vspace{4 cm}

2. Write the difference in AICs between the larger and smaller model in terms of log likelihoods. Feel free to refer to the log-likelihood as $\log L(\beta_{k+1}|\vv{y},\hat{\beta}_0,...,\hat{\beta}_k)$ (where $\beta_{k+1}$ should be replaced with something in each AIC). If $\textrm{AIC}_2-\textrm{AIC}_1<0$, what inequality does that give?

\vspace{6 cm}

3. Under the null that $\beta_{k+1}=0$, what is the asymptotic probability that $\textrm{AIC}_2-\textrm{AIC}_1<0$?

\vspace{3 cm}

4. If we view each step in the model selection as a likelihood test of whether $\textrm{AIC}_2<\textrm{AIC}_1$, what is the $\alpha$ level we are using for each test?

\vspace{3 cm}

```{r, include=F, cache=T}
library(MASS)

# Checking this actually works correctly
n <- 100
nsims <- 10000

true_coefficients <- c(0.2, 0.0, 2, 0)
p <- length(true_coefficients)
prop_test_correct <- c()

for (i in 1:nsims) {
  X <- matrix(rnorm(n * p), n, p)
  y <- X %*% true_coefficients + rnorm(n)
  data <- data.frame(y, X)
  
  # Run forward selection
  selected_model <- lm(y ~ 1, data = data)
  selected_vars <- c()
  
  i <- 1
  while (i <= p) {
    if (length(selected_vars) == p) {
      break
    }
    if (!(names(data)[i + 1] %in% selected_vars)) {
      candidate_formula <- as.formula(paste("y ~", paste(selected_vars, collapse = " + "), "+", names(data)[i + 1]))
      candidate_model <- lm(candidate_formula, data = data)
      
      # If the coefficient shouldn't be added, check whether it is
      if (names(data)[i + 1] %in% names(data)[true_coefficients!=0]) {
        if (AIC(candidate_model) < AIC(selected_model)) {
          prop_test_correct <- c(prop_test_correct, 0)
        } else {
          prop_test_correct <- c(prop_test_correct, 1)
        }
      }
      
      # Compare AIC between the current and previously selected models
      if (AIC(candidate_model) < AIC(selected_model)) {
        selected_model <- candidate_model
        selected_vars <- c(selected_vars, names(data)[i + 1])
        i <- 1
      } else {
        i <- i + 1
      }
    } else {
      i <- i + 1
    }
  }
}
t.test(prop_test_correct)
```







