---
output: pdf_document
header-includes:
   - \usepackage{tcolorbox}
   - \usepackage{fancyhdr}
   - \usepackage[utf8]{inputenc}
---

\pagestyle{fancy}
\fancyhf{}
\rhead{Will Nickols}
\lhead{Section 8}
\rfoot{Page \thepage}

# Announcements
- Make sure to sign in on the [google form (linked here)](https://forms.gle/JGvZP8CPUhaefnLT6)
- Pset 7 due November 4 at 5 pm
- Midterm 2 November 11 through November 18

# Weighted least squares regression

*This question is based on an October 29th conversation with Skyler Wu.*

Consider a least squares model where, rather than weighting all residuals equally, we are going to assign different weights to different residuals.  That is, we want to minimize

$$\sum_{i=1}^n[w_i(Y_i-\hat{Y_i})]^2$$
Equivalently, letting $\bf{W}$ be a diagonal matrix of weights, letting $\vec{Y}=\mathbf{X}\vec{\beta}+\vec{\epsilon}$ with $\vec{\epsilon}\sim\textrm{MVN}_n(0, \sigma^2I_n)$, and using the fact that $\hat{\vec{Y}}=\mathbf{X}\hat{\vec{\beta}}$, we want to minimize

$$||\mathbf{W}(\vec{Y}-\mathbf{X}\hat{\vec\beta})||^2$$

Expanding and taking the derivative gives the following:
$$\begin{aligned}
0&=\frac{\partial}{\partial\hat{\vec{\beta}}}((\vec{Y}-\mathbf{X}\hat{\vec\beta})^T\mathbf{W}^T\mathbf{W}(\vec{Y}-\mathbf{X}\hat{\vec\beta}))\\
&=-2\mathbf{X}^T\mathbf{W}^T\mathbf{W}(\vec{Y}-\mathbf{X}\hat{\vec\beta})\\
\implies \mathbf{X}^T\mathbf{W}^T\mathbf{W}\mathbf{X}\hat{\vec\beta}&=\mathbf{X}^T\mathbf{W}^T\mathbf{W}\vec{Y}\\
\implies \hat{\vec\beta}&=(\mathbf{X}^T\mathbf{W}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}^T\mathbf{W}\vec{Y}
\end{aligned}$$

This is our new weighted least-squares regression $\hat{\vec\beta}$, which we will be studying in this problem.

Here are a few facts that will be useful here and on the homework:

- For matrices $\mathbf{A}, \mathbf{B},$ and $\mathbf{C}$, of allowable dimensions, $\bf A(B+C)=AB+AC$
- If $\mathbf{A}$ is of full column rank, $\mathbf{A}^T\mathbf{A}$ is symmetric and invertible
- If $\mathbf{A}$ is symmetric, and $\mathbf{B}$ is of allowable dimensions, $\mathbf{B}^T\mathbf{A}\mathbf{B}$ is symmetric
- For an invertible and symmetric matrix $\mathbf{A}$, $\mathbf{A}^{-1}=(\mathbf{A}^{-1})^T$
- For $\vec{Y}=\vec{c}+\mathbf{B}\vec{X}$ with $\vec{c}$ and $\mathbf{B}$ constant and $\vec{X}$ random, $E(\vec{Y})=\vec{c}+\mathbf{B}E(\vec{X})$
- $\textrm{Cov}(\vec{X})$ is an $n \times n$ matrix whose ${i,j}$ entry is $\textrm{Cov}(X_i,X_j)$
- For $\vec{Y}=\vec{c}+\mathbf{B}\vec{X}$ with $\vec{c}$ and $\mathbf{B}$ constant and $\vec{X}$ random, $\textrm{Cov}(\vec{Y})=\mathbf{B}\textrm{Cov}(\vec{X})\mathbf{B}^T$

1. Verify that using the usual weights for least squares regression, this formula reduces to the usual estimator for $\hat{\vec\beta}$.

2. Find the bias of $\hat{\vec\beta}$ for $\vec{\beta}$.

3. Find the variance-covariance matrix of $\hat{\vec\beta}$ in matrix form.  When will this equal the variance-covariance matrix of $\hat{\vec\beta}$ in OLS regression?

# Ridge, LASSO, optimizing $\lambda$, and $\beta$ trajectories

This question will deal with a data set of country-level statistics from [this source](https://www.gu.se/en/quality-government/qog-data/data-downloads/standard-dataset) with an explanation of the data encoding found [here](https://www.qogdata.pol.gu.se/data/codebook_std_jan22.pdf).  

A few useful columns:

- `spi_ospi`: Overall social progress index on 0-100 scale
- `mad_gdppc`: GDP per capita
- `wdi_internet`: Percent of population using the internet
- `wdi_birth`: Birth rate per 1000 people
- `wdi_chexppgdp`: Current health expenditures as percent of GDP
- `wdi_elerenew`: Percent of total electricity output that's renewable
- `wdi_lifexp`: Life expectancy at birth
- `wdi_wip`: Proportion of seats held by women in national parliaments
- `wdi_popurb`: Percentage of total population that is urban
- `wdi_imig`: Proportion of people born outside the country in which they live

```{r, cache=T, include=F}
countries <- read.csv("data/countries.csv")

# All coefficients of interest
lm1 <- lm(spi_ospi ~ (mad_gdppc + wdi_internet + wdi_birth + wdi_chexppgdp + 
                        wdi_elerenew + wdi_lifexp + wdi_wip + wdi_popurb + wdi_imig)^2, countries)

summary(lm1)
```

1. Find a well-tuned Ridge regression model via \texttt{cv.glmnet} for predicting `spi_ospi`: consider all main predictors above and all 2-way interactions of these predictors.

```{r, cache=T, warning=F}
library(glmnet)
set.seed(139)

# Variables to be used
columns <- c("mad_gdppc", "wdi_internet", "wdi_birth", "wdi_chexppgdp", 
             "wdi_elerenew", "wdi_lifexp", "wdi_wip", "wdi_popurb", "wdi_imig")

# TODO: Model matrix for glmnet

# TODO: Scale the model matrix

# TODO: Run cross validation
```

2. Plot the average MSE on the validation sets against the $\lambda$'s you considered in the previous part.  Report the best $\lambda$ and justify this choice using this plot.

```{r, cache=T}
# TODO: Plot MSE
```

3. Provide the $\hat{\beta}$ trajectory plot of the main effects from this model (plot each $\beta_j$ as a function of $\lambda$ as a line, and do this for all 11 main effects).  Interpret what you see in 2-3 sentences.

```{r, cache=T}
# TODO: Plot beta trajectories
```

4. Fit a well-tuned LASSO regression model: examine main effects of predictors and all their 2-way interactions.

```{r, cache=T}
# TODO: Run cross validation
```

5. Plot the average MSE on the validation sets against the $\lambda$'s you considered in the previous part.  Report the best $\lambda$ and justify this choice using this plot.

```{r, cache=T}
# TODO: Plot MSE
```

6. Provide the $\hat{\beta}$ trajectory plot of the main effects from this LASSO model (plot each $\beta_j$ as a function of $\lambda$ as a line, and do this for all 11 main effects).  Compare this to the ridge trajectories.

```{r, cache=T}
# TODO: Plot trajectories
```

7. Choose a best regularized/penalized regression model and briefly justify your choice.

```{r}
# TODO: Choose best model
```

# Penalization functions

Recall that for both Ridge and LASSO, we are trying to minimize something of the form:

$$\sum_{i=1}^n(Y_i-\hat{Y_i})^2+p(\hat{\vec{\beta}})$$

State whether the following functions could or couldn't be used as penalization functions for $\hat{\vec{\beta}}$.  If they could, provide a context in which this might be a useful penalization function; if not, explain why it would give undesired behavior.

1. $p(\hat{\vec{\beta}})=\sum_{i=1}^k\hat{\beta_i}$

2. $p(\hat{\vec{\beta}})=\sum_{i=1}^k\hat{\beta_i}^4$

3. $p(\hat{\vec{\beta}})=\sum_{i=1}^k\log(\hat{\beta_i})$

4. $p(\hat{\vec{\beta}})=\sum_{i=1}^k\log(|\hat{\beta_i}|)$

5. $p(\hat{\vec{\beta}})=\sum_{i=1}^k1/|\hat{\beta_i}|$

6. $p(\hat{\vec{\beta}})=-\sum_{i=1}^k1/|\hat{\beta_i}|$

7. What general requirements do we need for a penalization function?

8. Write a valid penalization function that we haven't studied before.

# Miscellaneous

1. For what $\lambda$s would LASSO and Ridge give the same model?

2. Below are four $\hat\beta$ trajectory plots.  Each comes from a data set with 50 data points.  One trajectory comes from data with no built-in correlation between the predictors; one comes from data with moderate and equal correlation among all the predictors; one comes from data with moderate random (but fixed) correlation among the predictors; and one is fake (and impossible).  Determine which is which.

```{r, cache=F, fig.width=8, fig.height=8, echo=F, warning=F}
library(ggpubr)
library(grid)
plot1 <- readRDS(file = "plot1.RDS")
plot2 <- readRDS(file = "plot2.RDS")
plot3 <- readRDS(file = "plot3.RDS")
plot4 <- readRDS(file = "plot4.RDS")
figure <- ggarrange(plotlist = list(plot1, plot2, plot3, plot4), nrow = 2, ncol = 2)+
  theme(plot.margin = margin(1,1,1,1, "cm"))
annotate_figure(figure, left = textGrob("log(lambda)", rot = 90, vjust = 1, gp = gpar(cex = 1.3)),
                  bottom = textGrob("betas", gp = gpar(cex = 1.3)))

```
