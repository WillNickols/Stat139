---
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage{tcolorbox}
- \usepackage{fancyhdr}
- \usepackage[utf8]{inputenc}
- \usepackage{wrapfig}
- \usepackage{amsmath}
- \usepackage{booktabs}
- \usepackage{esvect}
urlcolor: blue
---

\pagestyle{fancy}
\fancyhf{}
\rhead{Will Nickols}
\lhead{Section 8}
\rfoot{Page \thepage}

# Announcements

\begin{wrapfigure}{r}{0.12\textwidth}
  \centering
    \vspace*{-1.3cm}
    \includegraphics[width=\linewidth]{section_qr_code.png}
\end{wrapfigure}

Make sure to sign in on the [google form](https://forms.gle/xm1DfzuZFNcWU6fH8) (I send a list of which section questions are useful for which pset questions afterwards)

Pset 8 due Friday 11/10

Midterm

# Introductions
- One question or thought related to lecture last week (LASSO, Ridge, cross validation)

```{r, echo=F, warning=F, message=F, cache=F}
list.of.packages <- c("ggplot2", "glmnet", "MASS", "ggpubr", "grid", "dplyr")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

library(ggplot2)
library(glmnet)
library(MASS)
library(ggpubr)
library(grid)
library(dplyr)
```


# Weighted least squares regression

*This question is based on a conversation with Skyler Wu.*

Consider a least squares model where, rather than weighting all residuals equally, we are going to assign different weights to different residuals. That is, we want to minimize

$$\sum_{i=1}^n[w_i(Y_i-\hat{Y_i})]^2$$
Equivalently, letting $\bf{W}$ be a diagonal matrix of weights, letting $\vv{Y}=\mathbf{X}\vv{\beta}+\vv{\epsilon}$ with $\vv{\epsilon}\sim\textrm{MVN}_n(0, \sigma^2I_n)$, and using the fact that $\hat{\vv{Y}}=\mathbf{X}\hat{\vv{\beta}}$, we want to minimize

$$||\mathbf{W}(\vv{Y}-\mathbf{X}\hat{\vv\beta})||^2$$

Expanding and taking the gradient gives the following:
$$\begin{aligned}
0&=\frac{\partial}{\partial\hat{\vv{\beta}}}((\vv{Y}-\mathbf{X}\hat{\vv\beta})^T\mathbf{W}^T\mathbf{W}(\vv{Y}-\mathbf{X}\hat{\vv\beta}))\\
&=-2\mathbf{X}^T\mathbf{W}^T\mathbf{W}(\vv{Y}-\mathbf{X}\hat{\vv\beta})\\
\implies \mathbf{X}^T\mathbf{W}^T\mathbf{W}\mathbf{X}\hat{\vv\beta}&=\mathbf{X}^T\mathbf{W}^T\mathbf{W}\vv{Y}\\
\implies \hat{\vv\beta}&=(\mathbf{X}^T\mathbf{W}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}^T\mathbf{W}\vv{Y}
\end{aligned}$$

This is our new weighted least-squares regression $\hat{\vv\beta}$, which we will be studying in this problem.

Here are a few facts that will be useful here and on the homework:

- For matrices $\mathbf{A}, \mathbf{B},$ and $\mathbf{C}$, of allowable dimensions, $\bf A(B+C)=AB+AC$
- If $\mathbf{A}$ is of full column rank, $\mathbf{A}^T\mathbf{A}$ is symmetric and invertible
- If $\mathbf{A}$ is symmetric, and $\mathbf{B}$ is of allowable dimensions, $\mathbf{B}^T\mathbf{A}\mathbf{B}$ is symmetric
- For an invertible and symmetric matrix $\mathbf{A}$, $\mathbf{A}^{-1}=(\mathbf{A}^{-1})^T$
- For $\vv{Y}=\vv{c}+\mathbf{B}\vv{X}$ with $\vv{c}$ and $\mathbf{B}$ constant and $\vv{X}$ random, $E(\vv{Y})=\vv{c}+\mathbf{B}E(\vv{X})$
- $\textrm{Cov}(\vv{X})$ is an $n \times n$ matrix whose ${i,j}$ entry is $\textrm{Cov}(X_i,X_j)$
- For $\vv{Y}=\vv{c}+\mathbf{B}\vv{X}$ with $\vv{c}$ and $\mathbf{B}$ constant and $\vv{X}$ random, $\textrm{Cov}(\vv{Y})=\mathbf{B}\textrm{Cov}(\vv{X})\mathbf{B}^T$

1. Verify that using the usual weights for least squares regression, this formula reduces to the usual estimator for $\hat{\vv\beta}$.

In our usual least squares regression, $\mathbf{W}=\mathbf{I}$, so 

$$\hat{\vv\beta}=(\mathbf{X}^T\mathbf{I}^T\mathbf{I}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{I}^T\mathbf{I}\vv{Y}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\vv{Y}$$
as expected.

2. If $\mathbf{W}=c\mathbf{I}$ for some non-zero constant $c$, what is $\hat{\vv\beta}$? What does this say about when $\mathbf{W}$ is useful?

$$\hat{\vv\beta}=(c^2\mathbf{X}^T\mathbf{I}^T\mathbf{I}\mathbf{X})^{-1}c^2\mathbf{X}^T\mathbf{I}^T\mathbf{I}\vv{Y}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\vv{Y}$$
which means the matrix of weights only changes the fit coefficients if the weights are different.

3. Find the bias of $\hat{\vv\beta}$ for $\vv{\beta}$.

$$\begin{aligned}
E(\hat{\vv\beta})-\vv{\beta}&=E\left[(\mathbf{X}^T\mathbf{W}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}^T\mathbf{W}\vv{Y}\right]-\vv{\beta}\\
&=(\mathbf{X}^T\mathbf{W}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}^T\mathbf{W}E\left[\vv{Y}\right]-\vv{\beta}\\
&=(\mathbf{X}^T\mathbf{W}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}^T\mathbf{W}\mathbf{X}\vv{\beta}-\vv{\beta}\\
&=\vv{0}
\end{aligned}$$

4. Find the variance-covariance matrix of $\hat{\vv\beta}$ in matrix form. What does this reduce to when $\mathbf{W}=\mathbf{I}$? You may find it useful to let $\mathbf{A}=(\mathbf{X}^T\mathbf{W}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}^T\mathbf{W}$ throughout.

$$\begin{aligned}
\textrm{Cov}(\hat{\vv\beta})&=\textrm{Cov}\left[\mathbf{A}\vv{Y}\right]\\
&=\mathbf{A}\textrm{Cov}\left(\vv{Y}\right)\mathbf{A}^T\\
&=\mathbf{A}\textrm{Cov}\left(\vv{\epsilon}\right)\mathbf{A}^T\\
&=\sigma^2\mathbf{A}\mathbf{I}\mathbf{A}^T\\
&=\sigma^2\mathbf{A}\mathbf{A}^T\\
\end{aligned}$$

When $\mathbf{W}^T\mathbf{W}=I$, $\mathbf{A}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T$, so $$\textrm{Cov}(\hat{\vv\beta})=\sigma^2(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}=\sigma^2(\mathbf{X}^T\mathbf{X})^{-1}$$ which is the usual variance-covariance matrix for linear regression.

\newpage

# Ridge, LASSO, optimizing $\lambda$, and $\beta$ trajectories

These problems will deal with a dataset of country-level statistics from [UNdata](https://data.un.org/), [Varieties of Democracy](https://v-dem.net/data/the-v-dem-dataset/), and the [World Bank](https://data.worldbank.org/indicator/AG.LND.PRCP.MM).

```{r, cache=T, warning=FALSE, echo=F}
countries <- read.csv("data/country_stats.csv", check.names = F)
countries_2010 <- countries[countries$Year == 2010,]

countries_subset <- countries_2010[!is.na(log2(countries_2010$`GDP per capita (US dollars)`)) & 
                                   !is.na(countries_2010$`Urban population (percent)`) &
                                   !is.na(countries_2010$`Population aged 60+ years old (percentage)`) &
                                     !is.na(countries_2010$`Arable land (% of total land area)`) &
                                     !is.na(countries_2010$`Supply per capita (gigajoules)`) &
                                     !is.na(countries_2010$`Unemployment rate - Total`) &
                                     !is.na(countries_2010$`Tourist/visitor arrivals (thousands)`),]

colnames(countries_subset) <- case_when(colnames(countries_subset) == "GDP per capita (US dollars)" ~ "GDP",
                                        colnames(countries_subset) == "Urban population (percent)" ~ "Urban",
                                        colnames(countries_subset) == "Population aged 60+ years old (percentage)" ~ "60+",
                                        colnames(countries_subset) == "Arable land (% of total land area)" ~ "Arable",
                                        colnames(countries_subset) == "Supply per capita (gigajoules)" ~ "Energy",
                                        colnames(countries_subset) == "Unemployment rate - Total" ~ "Unemployment",
                                        colnames(countries_subset) == "Tourist/visitor arrivals (thousands)" ~ "Tourists",
                                        TRUE ~ colnames(countries_subset)
                                        )
```

1. First, we'll fit a well-tuned Ridge regression model via \texttt{cv.glmnet} for predicting log2 GDP per capita from a country's urban population, its proportion of people 60+, its arable land, its energy supply, its unemployment rate, and its number of tourists and visitors. We'll perform 10-fold cross validation to choose the optimal $\lambda$. What is the optimal model?

```{r, cache=F, echo=F}
set.seed(139)

lm1 <- lm(log2(GDP) ~ (Urban + `60+` + Arable + Energy + Unemployment + Tourists)^2, countries_subset)

# Model matrix for glmnet
X = model.matrix(log2(GDP) ~ (Urban + `60+` + Arable + Energy + Unemployment + Tourists)^2, countries_subset)[,-1] # Drop the intercept

# Scaling X is usually a good idea when penalizing betas
X <- scale(X)
y <- unlist(lm1$model["log2(GDP)"])

# Run cross validation
ridges=cv.glmnet(X, y, alpha=0, lambda = exp(seq(-10,10,0.1)), nfolds=10)

print(ridges)
```
The best ridge regression model is one with a $\lambda$ of 0.011. A $\lambda$ up to 0.091 might also be reasonable.

2. The following is a plot of MSE on the validation sets against the $\lambda$'s from the previous part. Justify the previous $\lambda$ with this plot.

```{r, cache=F, echo=F, fig.width=4.5, fig.height=4.5, fig.align='center'}
plot(ridges)
```

The best $\lambda$ is 0.011, which corresponds to $\log(0.011)=-4.51$ in the plot. Values of $\lambda$ slightly above this might also help in preventing overfitting.

3. The following are the $\hat{\beta}$ trajectories of the main (non-interaction) effects from this model. Interpret what you see in 2-3 sentences.

```{r, cache=F, fig.width=4.5, fig.height=4.5, fig.align='center', echo=F}
# Fit for many lambdas
ridges_for_plot=glmnet(X, unlist(lm1$model["log2(GDP)"]), alpha=0, lambda=exp(seq(-10,10,0.1)))

# Plot trajectories
matplot(log(ridges_for_plot$lambda),
        t(ridges_for_plot$beta[-grep(":", rownames(ridges_for_plot$beta)),]), # Remove interactions
        type="l",
        xlab = "log(lambda)", 
        ylab = "betas")
```
We can see that the estimates vary a lot for low values of $\lambda$, but coefficients shrink to zero asymptotically. However, as $\lambda$ grows, the $\hat\beta$s never reach zero exactly as expected in ridge regression.

4. Next, we'll fit a well-tuned LASSO regression model for the same question. We'll perform 10-fold cross validation to choose the optimal $\lambda$. What is the optimal model? Is this consistent with the plot?

```{r, cache=F, fig.width=4.5, fig.height=4.5, fig.align='center', echo=F}
# Run cross validation
lassos=cv.glmnet(X, y, alpha=1, lambda = exp(seq(-10,10,0.1)), nfolds = 10)

plot(lassos)
print(lassos)
```
The best $\lambda$ is 0.00166, which corresponds to $\log(0.00166)=-6.4$ in the plot. Values of $\lambda$ slightly above this might also help in preventing overfitting.

5. The following are the $\hat{\beta}$ trajectories of the main (non-interaction) effects from this model. Compare these to the ridge trajectories.

```{r, cache=F, fig.width=4.5, fig.height=4.5, fig.align='center', echo=F}
lasso_for_plot = glmnet(X, unlist(lm1$model["log2(GDP)"]), alpha=1, lambda=exp(seq(-10,10,0.1)))
matplot(log(lasso_for_plot$lambda),
        t(lasso_for_plot$beta[-grep(":", rownames(lasso_for_plot$beta)),]), # Remove interactions
        type="l", 
        xlab = "log(lambda)", 
        ylab = "betas")
```
The estimates vary a lot for low values of $\lambda$, and they even increase in magnitude occasionally, indicating collinearity. However, as $\lambda$ grows, more and more snap to 0. Compared to ridge regression, these coefficients decrease more sporadically but actually become 0 rather than just approaching 0.

6. What is the best regularized/penalized regression model?

```{r, echo=F}
df <- data.frame(`Ridge, lambda=0.011`=min(ridges$cvm), `LASSO, lambda=0.0017`=min(lassos$cvm), check.names = F)
rownames(df) <- "MSE"
df
```
These are the minimum means of cross validated error (the cross validated MSE using the optimal $\lambda$). Ridge has a slightly lower MSE on cross-validation, so ridge with $\lambda=0.011$ is the best regularized model.

\newpage

# Penalization functions

Recall that for both Ridge and LASSO, we are trying to minimize something of the form:

$$\sum_{i=1}^n(Y_i-\hat{Y_i})^2+p(\hat{\vv{\beta}})$$

State whether the following functions could or couldn't be used as penalization functions for $\hat{\vv{\beta}}$. If so, provide a context in which this might be a useful penalization function; if not, explain why it would give undesired behavior.

1. $p(\hat{\vv{\beta}})=\sum_{i=1}^k\hat{\beta_i}$

No: the $\hat\beta_i$ would just get more and more negative.

2. $p(\hat{\vv{\beta}})=\sum_{i=1}^k\hat{\beta_i}^4$

Yes: you could use this if you wanted to very strongly penalize $\hat\beta$s with large magnitudes.

3. $p(\hat{\vv{\beta}})=\sum_{i=1}^k\log(\hat{\beta_i})$

No: negative $\hat{\beta_i}$ would break this.

4. $p(\hat{\vv{\beta}})=\sum_{i=1}^k\log(|\hat{\beta_i}|)$

No: the $\hat{\beta_i}$ would always go to 0 since that would give $-\infty$ loss.

5. $p(\hat{\vv{\beta}})=\sum_{i=1}^k1/|\hat{\beta_i}|$

No: larger $\hat{\beta}$s give a smaller loss.

6. $p(\hat{\vv{\beta}})=-\sum_{i=1}^k1/|\hat{\beta_i}|$

Still no: the $\hat{\beta_i}$ would always go to 0 since that would give $-\infty$ loss.

7. What general requirements do we need for a penalization function?

The loss should increase (or remain 0) with coefficients of increasing magnitude, and it should have a (preferably global) minimum.

8. Write a valid penalization function that we haven't studied before.

One example would be $$p(\hat{\vv{\beta}})=\sum_{i=1}^kf(\hat\beta_i)\textrm{ with } f(\hat\beta_i)=\begin{cases} 
      0 & |\hat\beta_i|\leq \delta \\
      |\hat\beta_i| & \textrm{Otherwise} \\
   \end{cases}$$
for some $\delta$, which applies no penalty once a $\hat\beta_i$ is within $\delta$ of 0.

\newpage

# Miscellaneous

1. For what $\lambda$s would LASSO and Ridge give the same model?

$\lambda=0,\infty$ since there would be either no penalization or the model would just be $\bar{Y}$.

2. Below are four $\hat\beta$ trajectory plots. Each comes from a data set with 50 data points. One trajectory comes from data with no built-in correlation between the predictors; one comes from data with moderate and equal correlation among all the predictors; one comes from data with moderate random (but fixed) correlation among the predictors; and one is fake (and impossible). Determine which is which.

```{r, include=F, cache=F}
par(oma = c(0,0,0,0), mar=c(1,2,1,1))
set.seed(139)

n <- 50
X <- matrix(rnorm(n * 8, 0, 1), ncol = 8)
betas = c(1, 2, 2, 3, 4, 0, 0, 0)
y <- X %*% betas

ridges_for_plot=glmnet(X, y, alpha=0, lambda=exp(seq(-3,10,0.1)))

# Plot trajectories
matplot(log(ridges_for_plot$lambda),
        t(ridges_for_plot$beta),type="l", # Remove interactions
        xlab = "log(lambda)", 
        ylab = "betas")

plot1 <- recordPlot()
plot.new() ## clean up device

Sigma <- matrix(0.9, nrow = 8, ncol = 8)
diag(Sigma) <- 1
X <- mvrnorm(n = n, c(1:5, 0, 0, 0), Sigma)
y <- X %*% betas

ridges_for_plot=glmnet(X, y, alpha=0, lambda=exp(seq(-3,10,0.1)))

# Plot trajectories
matplot(log(ridges_for_plot$lambda),
        t(ridges_for_plot$beta),type="l", # Remove interactions
        xlab = "log(lambda)", 
        ylab = "betas")

plot2 <- recordPlot()
plot.new() ## clean up device

Sigma <- matrix(runif(64, 0, 0.5), nrow = 8, ncol = 8)
diag(Sigma) <- 1
Sigma[lower.tri(Sigma)] = t(Sigma)[lower.tri(Sigma)]
X <- mvrnorm(n = n, c(1:5, 0, 0, 0), Sigma)
y <- X %*% betas

ridges_for_plot=glmnet(X, y, alpha=0, lambda=exp(seq(-3,10,0.1)))


# Plot trajectories
matplot(log(ridges_for_plot$lambda),
        t(ridges_for_plot$beta),type="l", # Remove interactions
        xlab = "log(lambda)", 
        ylab = "betas")

plot3 <- recordPlot()
plot.new() ## clean up device

Sigma <- matrix(runif(64, 0, 0.5), nrow = 8, ncol = 8)
diag(Sigma) <- 1
Sigma[lower.tri(Sigma)] = t(Sigma)[lower.tri(Sigma)]
X <- mvrnorm(n = n, c(1:5, 0, 0, 0), Sigma)
y <- X %*% betas

ridges_for_plot=glmnet(X, y, alpha=0, lambda=exp(seq(-3,10,0.1)))


# Plot trajectories
matplot(log(ridges_for_plot$lambda),
        t(ridges_for_plot$beta) + matrix(c(rep(0, dim(ridges_for_plot$beta)[2] - 
                                                   length(seq(-5,2,0.07))), dnorm(seq(-5,2,0.07))), 
                                         ncol = 8, 
                                           nrow = dim(ridges_for_plot$beta)[2]) * 1.5,
        type="l", # Remove interactions
        xlab = "log(lambda)", 
        ylab = "betas")

plot4 <- recordPlot()
plot.new() ## clean up device

saveRDS(plot3, "plot1.RDS")
saveRDS(plot1, "plot2.RDS")
saveRDS(plot4, "plot3.RDS")
saveRDS(plot2, "plot4.RDS")
```

```{r, cache=F, fig.width=8, fig.height=8, echo=F, warning=F}
plot1 <- readRDS(file = "plot1.RDS")
plot2 <- readRDS(file = "plot2.RDS")
plot3 <- readRDS(file = "plot3.RDS")
plot4 <- readRDS(file = "plot4.RDS")
figure <- ggarrange(plotlist = list(plot1, plot2, plot3, plot4), nrow = 2, ncol = 2)+
  theme(plot.margin = margin(1,1,1,1, "cm"))
annotate_figure(figure, left = textGrob("betas", rot = 90, vjust = 1, gp = gpar(cex = 1.3)),
                  bottom = textGrob("log(lambda)", gp = gpar(cex = 1.3)))

```
The first plot is the random correlation as seen by its peaks rising and falling more sporadically. The second is the no correlation plot because the slopes mostly fall independently of each other. The third is impossible because all the slopes rise at the beginning (but at least one must fall for the other to rise). The fourth is the strong correlation as seen by the fact that all the slopes are about the same after $\lambda=e^2$.

