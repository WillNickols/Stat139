---
output: pdf_document
header-includes:
   - \usepackage{tcolorbox}
   - \usepackage{fancyhdr}
   - \usepackage[utf8]{inputenc}
---

\pagestyle{fancy}
\fancyhf{}
\rhead{Will Nickols}
\lhead{Section 7}
\rfoot{Page \thepage}

# Announcements
- Make sure to sign in on the [google form (linked here)](https://forms.gle/JGvZP8CPUhaefnLT6)
- Pset 6 due October 28 at 5 pm
- Project proposal due October 28 at 5 pm

# From RSS to BIC

When describing Bayes Information Criterion, the lecture notes leave the equation at $$\textrm{BIC} = 2\ln(g(\textrm{SSE})) + (p+1)\log(n)$$ where $g$ is some mysterious likelihood function.  Wikipedia [asserts](https://en.wikipedia.org/wiki/Bayesian_information_criterion#Gaussian_special_case) (with citation but without proof) that for a Gaussian model, $\textrm{BIC} = n\ln(\textrm{RSS}/n)+p\ln(n)$ where their $p$ includes the intercept.  In this problem, we'll derive the result for ourselves in our usual notation.

1. First, recall that for a multiple regression model, $Y_i=\beta_0+\beta_1X_{1,i}+...+\beta_pX_{p, i}+\epsilon_i$ with $\epsilon_i\sim\mathcal{N}(0, \sigma^2)$.  Also recall that for this distributional assumption, $\hat{\vec{\beta}}$ is the set of parameters that maximize the likelihood function of the whole model.  Lastly, recall that in a multiple regression model, the maximum likelihood estimate for the residual variance is $\hat\sigma^2=\frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{n}$ (and note that this is different from our unbiased estimator).  Write the maximized likelihood function for the observed data as a function of $\hat{y_i}$, $y_i$, and $\hat\sigma^2$.

$$\prod_{i=1}^n\frac{1}{\hat\sigma\sqrt{2\pi}}\exp\left[-\frac{1}{2}\frac{(y_i - \hat{y_i})^2}{\hat\sigma^2}\right]$$

2. Write the maximized log likelihood function of the observed data as a function of the residual sum of squares ($\textrm{RSS}$).  (You will find there are two terms that are constant regardless of the predictors; these can be dropped because we are only interested in comparing AIC between models.)

$$\begin{aligned}
\log(\hat{L})&=\sum_{i=1}^n-\log\left(\sqrt{\frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{n}}\right) - \log(\sqrt{2\pi}) -\frac{1}{2}\frac{(y_i - \hat{y_i})^2}{\frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{n}}\\
&=\sum_{i=1}^n-\frac{1}{2}\log\left(\frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{n}\right) - \log(\sqrt{2\pi})-\frac{n}{2}\frac{(y_i - \hat{y_i})^2}{\sum_{i=1}^n(y_i-\hat{y_i})^2}\\
&=-\frac{n}{2}\left[\log\left(\frac{\textrm{RSS}}{n}\right)\right] - n\log(\sqrt{2\pi})-\frac{n}{2}
\end{aligned}$$

Since $n\log(\sqrt{2\pi})$ and $\frac{n}{2}$ do not depend on the predictors, we can drop these terms, so we end up with $$-\frac{n}{2}\left[\log\left(\frac{\textrm{RSS}}{n}\right)\right]$$

Note that the part that actually mattered from the likelihood function was the normalizing constant!

3. Find the Bayes Information Criterion (where the Bayes Information Criterion is $(p+1)\ln(n)-2\ln(\hat{L})$ and $\hat{L}$ is the maximized likelihood function).

$$(p+1)\ln(n)+n\log\left(\frac{\textrm{RSS}}{n}\right)$$
In the original formulation of BIC, the first term should actually be $(p+2)\ln(n)$ because we are fitting $p$ predictors, an intercept, and a residual standard error.  However, this only changes the resulting BIC by a constant for all models of this type, so we (and R) drop it.

```{r, cache=T, warning=FALSE}
# Fit example model
countries <- read.csv("data/countries.csv")
model1 <-lm(wdi_araland~poly(wdi_precip, 2, raw = TRUE), countries)

# R's AIC
extractAIC(model1, k=log(length(model1$residuals)))[2]

# AIC by hand
n <- length(model1$residuals)
3 * log(n) + n * log(sum(model1$residuals^2)/n)
```

# The Red Queen's $R^2$

1. Recall the formula for adjusted $R_{adj}^2$: $$1-(1-R^2)\frac{n-1}{n-p-1}$$  Consider a model with $p$ predictors where the unadjusted $R^2$ is $R^2_p$.  What unadjusted $R^2_i$ would a model with $i$ predictors need to have so that the adjusted $R_{adj}^2$ remains unchanged?

For the adjusted $R_{adj}^2$ to remain the same, we need:

$$1-(1-R_p^2)\frac{n-1}{n-p-1}=1-(1-R_i^2)\frac{n-1}{n-i-1}\implies$$
$$R_i^2=1-(1-R_p^2)\frac{n-i-1}{n-p-1}$$

2. For what $p$ does adding an additional predictor require the smallest increase in unadjusted $R^2$ for the adjusted $R^2$ to remain the same?  For what $p$ does adding an additional predictor require the greatest increase?  What are the increases in unadjusted $R^2$ in both cases?

The difference in unadjusted $R^2$ required is
$$1-(1-R_p^2)\frac{n-(p+1)-1}{n-p-1}-R_{p}^2=(1-R_p^2)\left[1-\frac{n-p-2}{n-p-1}\right]$$
Since everything else is a multiplicative or additive constant, this is minimized when $\frac{n-p-2}{n-p-1}$ is largest, which is when $p=0$.  In particular, since $R^2_0=0$, a single predictor needs to give an unadjusted $R^2$ of $1-\frac{n-2}{n-1}=\frac{1}{n-1}$ to give an adjusted $R^2$ of 0.

Likewise, the difference is maximized when $\frac{n-p-2}{n-p-1}$ is smallest, which happens when $p$ is at the largest value for which the adjusted $R^2$ of the next $p$ is defined: $p=n-3$.  For this $p$, the difference in $R^2$ needs to be $$\frac{1}{2}(1-R_p^2)$$ for the adjusted $R^2$ to be unchanged.

Also, note that for all $p$ the required difference is maximized when $R_p$ is 0.

# Step procedures and cross validation

1. Given the following table, find the model produced by forward selection using an ESS $F$-test and starting from a model with only an intercept.  (You should be able to do this with only a single test.)

\begin{center}
\begin{tabular}{ccc}
\bf{Model} & \bf{Residual sum} & \bf{Degrees} \\
\bf{Variables} & \bf{of squares} & \bf{of freedom} \\
\hline
None & 7,200 & 38 \\
$X_1$ & 6,600 & 37 \\
$X_2$ & 6,980 & 37 \\
$X_3$ & 6,760 & 37 \\
\end{tabular}
\end{center}

Since the model with $X_1$ has the smallest residual sum of squares, we will test for it being a better predictive model than the intercept-only model.  Our test statistic is $$F=\frac{(7200 - 6600)/1}{6600/37}\approx1.17$$ which we test using a $F_{1, 37}$ distribution.
```{r, cache=TRUE}
f_stat <- (7200 - 6600)/(6600/37)
1 - pf(f_stat, 1, 37)
```
We get a p-value of $0.074>0.05$, so we fail to reject the null and conclude that $X_1$, $X_2$, and $X_3$ add no predictive power on their own.

\newpage

The rest of this section will deal with a data set of country-level statistics from [this source](https://www.gu.se/en/quality-government/qog-data/data-downloads/standard-dataset) with an explanation of the data encoding found [here](https://www.qogdata.pol.gu.se/data/codebook_std_jan22.pdf).  

A few useful columns:

- `mad_gdppc`: GDP per capita
- `bi_fishes`: Number of endangered fish species
- `bi_fungi`: Number of endangered fungi species
- `bi_mammals`: Number of endangered mammal species
- `bi_reptiles`: Number of endangered reptile species
- `bi_molluscs`: Number of endangered mollusc species
- `bi_othinverts`: Number of other endangered invertebrate species

2. The next three questions will ask you to run forward, backward, and both-direction variable selection procedures.  Briefly glance ahead and predict which model will have the highest $R^2$.

We expect the backwards variable selection procedure to give a model with the highest $R^2$ because it starts with the largest model and is therefore most likely to overfit.  There is essentially a multiple hypothesis testing scenario taking place here in which the backwards variable selection procedure requires us to fail to reject the null far more times to end up with a model as small as that of the forward or both-direction procedures.

3. Run a forward variable selection procedure to predict log GDP per capita from endangered species statistics starting with an intercept only model and using an upper scope of all the two-way interaction terms for the variables listed above.  Report this model’s coefficient estimates, R^2, and AIC.

```{r, cache=T, echo=F}
countries <- read.csv("data/countries.csv")
```

```{r, cache=TRUE}
lm1 <- lm(log(mad_gdppc) ~ 1, countries)
step_model_1 <- step(lm1, 
                     scope = list(upper = formula(lm(log(mad_gdppc) ~ bi_fishes * bi_fungi * bi_mammals * bi_molluscs * 
                                                       bi_othinverts * bi_reptiles, countries))), 
                     direction = "forward", trace = F)
summary(step_model_1)$coefficients
summary(step_model_1)$r.squared
AIC(step_model_1)
```

4. Run a backwards variable selection procedure to predict log GDP per capita from endangered species statistics starting with all interaction terms of the variables listed above and using a lower bound of an intercept-only model.  Report this model’s coefficient estimates, R^2, and AIC.

```{r, cache=TRUE}
step_model_2 <- step(lm(log(mad_gdppc) ~ bi_fishes * bi_fungi * bi_mammals * bi_molluscs * 
                          bi_othinverts * bi_reptiles, countries), 
                     scope = list(lower = formula(lm(log(mad_gdppc) ~ 1, countries))), 
                     direction = "backward", trace = F)
summary(step_model_2)$coefficients
summary(step_model_2)$r.squared
AIC(step_model_2)
```
5. Run a both-direction variable selection procedure to predict log GDP per capita from endangered species statistics starting with a model including all variables listed above (but no interactions) and using a lower bound of an intercept-only model and an upper bound of a model with all the interaction terms.  Report this model’s coefficient estimates, R^2, and AIC.

```{r, cache=TRUE}
step_model_3 <- step(lm(log(mad_gdppc) ~ bi_fishes + bi_fungi + bi_mammals + bi_molluscs + 
                          bi_othinverts + bi_reptiles, countries), 
                     scope = list(lower = formula(lm(log(mad_gdppc) ~ 1, countries)),
                                  upper = formula(lm(log(mad_gdppc) ~ bi_fishes * bi_fungi * bi_mammals * 
                                                       bi_molluscs * bi_othinverts * bi_reptiles, 
                                                     countries))), 
                     direction = "both", trace = F)
summary(step_model_3)$coefficients
summary(step_model_3)$r.squared
AIC(step_model_3)
```
6. Based on AIC, which model is the best?  Why didn't the other procedures find the same model?

The forward procedure found a model with the lowest AIC, so it produced the best model.  Step-wise variable selection is prone to getting stuck in local minima, so it is possible for different starting models to converge on different final models.

7. Recall from last week that we looked at various models incorporating the following variables:
- `wdi_araland`: Arable land (\% of land area)
- `wdi_precip`: Average annual precipitation (mm per year)

Run $k$-fold cross validation with $k=10, 20, 50$ to estimate out-of-sample RMSE for a LOESS model and a degree 2 polynomial model to predict the proportion of arable land from the country's average annual precipitation.  Which model performs better for each $k$?

```{r, warning=F, cache=T}
library(caret)
set.seed(139)

poly_model <- vector(length = 3)
i <- 1
for (ncross in c(10, 20, 50)) {
  poly_model[i] <- train(wdi_araland ~ poly(wdi_precip, 2, raw = TRUE), data = countries,
      method = "lm",
      trControl = trainControl(method = "cv", number = ncross), 
      na.action = na.omit)$results[2]
  i <- i + 1
}

loess_model <- vector(length = 3)
i <- 1
for (ncross in c(10, 20, 50)) {
  loess_model[i] <- train(wdi_araland ~ wdi_precip, data = countries,
      method = "gamLoess", tuneGrid = expand.grid(span = 1,degree=1),
      trControl = trainControl(method = "cv", number = ncross), 
      na.action = na.omit)$results[3]
  i <- i + 1
}

cbind(poly_model, loess_model)
```
The polynomial model performs better than the LOESS model for all $k$.  The RMSE decreases for higher $k$ because a smaller proportion of the data is reserved for testing, so more data is used to fit the model.  (A higher $k$ makes cross validation take longer though, which can be problematic for larger models.)







